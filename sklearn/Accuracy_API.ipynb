{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn API Experimentation\n",
    "## Accuracy\n",
    "\n",
    "Accuracy is the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "There are a few different ways to compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (891, 11)\n",
      "y Shape:  (891,)\n",
      "X columns:\n",
      " ['PassengerId' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch' 'Ticket' 'Fare'\n",
      " 'Cabin' 'Embarked']\n",
      "y name: Survived\n"
     ]
    }
   ],
   "source": [
    "# Load Titanic Data\n",
    "%cd -q ../projects/titanic\n",
    "%run LoadTitanicData.py\n",
    "%cd -q -\n",
    "\n",
    "# X: features\n",
    "# y: target variable\n",
    "print('X Shape: ', X.shape)\n",
    "print('y Shape: ', y.shape)\n",
    "print('X columns:\\n', X.columns.values)\n",
    "print('y name:',y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model\n",
    "drop_fields = ['Name', 'Sex', 'Ticket', 'Cabin', \n",
    "               'Embarked', 'PassengerId', 'Age']\n",
    "\n",
    "# Remove all non-numeric fields and PassengerId (1st iteration only)\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "X.dtypes\n",
    "\n",
    "# create train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.30, stratify=y, random_state=10)\n",
    "\n",
    "# Create instance of LogisticRegression estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "base_model = LogisticRegression()\n",
    "\n",
    "# Build Model on training data\n",
    "# information about the fitted model is returned\n",
    "model_info = base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6305970149253731\n",
      "0.6305970149253731\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "predictions = base_model.predict(X_test)\n",
    "\n",
    "# Compute accuarcy using sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Compute accuracy manually to be sure we understand accuracy_score()\n",
    "print((y_test == predictions).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy_score() gave us the same value as determining the mean number of times the prediction was correct.\n",
    "\n",
    "Although it may seem confusing at first to see mean() used to compute the percentage of True values in a boolean collection, it is a commonly used idiom.\n",
    "\n",
    "To fully understand what is happening, it is often helpful to look at the data types of the objects.  Let's look at the last line of code above in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Collection Type:  <class 'numpy.ndarray'>\n",
      "Predictions Value Type:       int64\n",
      "Predictions Values:           [0 1]\n",
      "y_test Collection Type:       <class 'pandas.core.series.Series'>\n",
      "y_test Value Type:            int64\n",
      "y_test Values:                [0 1]\n",
      "Comparison Collection Type:   <class 'pandas.core.series.Series'>\n",
      "Comparison Value Type:        bool\n",
      "Comparison Values:            [ True False]\n",
      "Accuracy:                     0.6306\n"
     ]
    }
   ],
   "source": [
    "print('Predictions Collection Type: ', type(predictions))\n",
    "print('Predictions Value Type:      ', predictions.dtype)\n",
    "print('Predictions Values:          ', np.unique(predictions))\n",
    "print('y_test Collection Type:      ', type(y_test))\n",
    "print('y_test Value Type:           ', y_test.dtype)\n",
    "print('y_test Values:               ', y_test.unique())\n",
    "print('Comparison Collection Type:  ', type(predictions == y_test))\n",
    "print('Comparison Value Type:       ', (predictions == y_test).dtype)\n",
    "print('Comparison Values:           ', (predictions == y_test).unique())\n",
    "print('Accuracy:                     {:.4f}'.format((y_test == predictions).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "* predictions, returned from predict(), is a numpy array of integers\n",
    "* y, the response (or target) variable, is a Pandas Series\n",
    "* comparisons between numpy arrays and Pandas Series are allowable\n",
    "* this comparison results in a Pandas Series of type bool\n",
    "* taking the mean of a boolean collection gives the percentage of True values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "For binary classification problems such as this, the following terms are often used:\n",
    "<pre>\n",
    "TP = True  Positive  \n",
    "FP = False Positive  (also called Type 1 Error)\n",
    "TN = True  Negative  \n",
    "FN = False Negative  (also called Type 2 Error)\n",
    "</pre>\n",
    "\n",
    "Where \"positive\" in this example means \"Survived\".\n",
    "\n",
    "TP means we predicted survived and that passenger did survive.  \n",
    "FP means we predicted survived and the passenger did not survive.  \n",
    "TN means we predicted not-survived and the passenger did not survive.  \n",
    "FN means we predicted not-survived and the passenger did survive.\n",
    "\n",
    "Accuracy is the number of true predictions divided by the total number of predictions:\n",
    "(TN + TP) / (TN + FP + FN + TP)\n",
    "\n",
    "\n",
    "For a binary classification problem, sklearn represents this as a [Confusion Matrix](http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TN FP\n",
      " FN TP\n"
     ]
    }
   ],
   "source": [
    "# sklearn confusion matrix\n",
    "print(\"\",\"TN\", \"FP\\n\",\"FN TP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126  39]\n",
      " [ 60  43]]\n"
     ]
    }
   ],
   "source": [
    "# For instructional purposes, let's derive the confusion matrix ourselves\n",
    "TN = ((y_test == 0) & (predictions == 0)).sum()\n",
    "FP = ((y_test == 0) & (predictions == 1)).sum()\n",
    "FN = ((y_test == 1) & (predictions == 0)).sum()\n",
    "TP = ((y_test == 1) & (predictions == 1)).sum()\n",
    "\n",
    "my_confusion_matrix = np.array([[TN, FP],[FN, TP]])\n",
    "print(my_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126  39]\n",
      " [ 60  43]]\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn to compute the confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our hand-coded and sklearn confusion matrix results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6306\n"
     ]
    }
   ],
   "source": [
    "# Compute Accuracy from Confusion Matrix\n",
    "print('Accuracy: {:.4f}'.format((TN+TP)/(TN + FP + FN + TP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we got the same accuarcy as before, about 63.1%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
