{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Iterative Model Development<br/>*The ML Model Building Process*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Goals\n",
    "1. Describe Iterative Model Development.\n",
    "2. Describe how to Compare Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Introduction to Iterative Model Development Series\n",
    "\n",
    "These notebooks are designed to be used with Jupyter Lab and the [Table of Contents](https://github.com/jupyterlab/jupyterlab-toc) plugin.  This plugin allows an outline of the notebook to be viewed in the left-hand panel with the ability to jump to any section. \n",
    "\n",
    "This is the first in a series of notebooks about building Machine Learning models with Scikit Learn.\n",
    "* Only Supervised Learning is considered.\n",
    "* Prediction accuracy is emphasized over model interpretability.\n",
    "* Last notebook in series will present a simple, interpretable, and well performing model.\n",
    "\n",
    "The content is intended to cover common questions a beginner may have, after having made an initial attempt to learn Python, Pandas, Scikit Learn and Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Machine Learning Models\n",
    "\n",
    "A model is a simplified representation of a system or process.\n",
    "\n",
    "A Machine Learning (ML) Model is created by a machine.  The machine being the computer running a learning algorithm.\n",
    "\n",
    "A ML Model is often a representation of a data generation process.  The Model can be used to predict new data, or understand existing data.\n",
    "\n",
    "Creating a ML Model is a semi-automated data driven process.  A reasonable model can often be created without knowing much about the domain of the data.  A better model can often be created by extracting features and applying data transformations specific to the domain of the data.\n",
    "\n",
    "A learning algorithm \"learns\" or \"fits\" the data it is presented with to create a model.  A model represents data in a manner specific to the learning algorithm that created it.  For example a Linear Regression model represents data as a line.  A Decision Tree Classifier, represents data as a hierarchical set of if-then-else rules.\n",
    "\n",
    "Models with simple internal data representations, such as Linear Regression and Decision Trees, can have their internal representation examined by an end-user to provide insight into the user's data.  Models which have complex internal representations are used to make the best predictions possible.\n",
    "\n",
    "The fitting process is performed by an algorithm which optimizes the model's internal representation of the data.  In Scikit Learn terminology, fitting the data to create a model is called \"minimizing the objective function\". The objective function is a measure of the \"distance\" between the model's representation of the data and the actual data. When this distance has been minimized, a model has been created which has \"learned\" or \"fit\" the data.\n",
    "\n",
    "In general, the metric used by a learning algorithm to best fit a model to the data, is not the same as the metric used by an end-user to evaluate the usefulness of that model for their particular application.  In Scikit Learn terminology, the \"objective function\" used by the learning algorithm is usually different than the \"scoring\" function selected by the end-user.\n",
    "\n",
    "The scoring function should be applied to data that was not used to build the model. The practical value of a predictive model depends upon how well it can predict on new data.\n",
    "\n",
    "In Scikit Learn terminology:\n",
    "1. An **estimator** is a Python object which:\n",
    "   * has an algorithm to fit data\n",
    "   * has internal data structures to hold the results of having fit the data\n",
    "2. **estimator.fit()** fits the data (it creates the model)\n",
    "3. **estimator.predict()** uses the model to make new predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## The Model Building Process\n",
    "This series of notebooks is about how to build ML models in general.  For concreteness, the Titanic data set from Kaggle will be used.\n",
    "\n",
    "Some of the model building techniques presented in the later notebooks of this series are overkill for a data set as simple as the Titanic. However they are likely to be useful for larger and more complex data sets. \n",
    "\n",
    "Model Building Process:  \n",
    "1. Identify and quantify goal\n",
    "2. Perform Exploratory Data Analysis (EDA)\n",
    "3. Build initial model and evaluate its usefulness\n",
    "4. Create a better model using better:\n",
    "    * Exploratory Data Analysis \n",
    "    * Preprocessing (data transformations)\n",
    "    * Feature Extraction\n",
    "    * Tuning of hyperparameters\n",
    "    * Other estimators (other model building algorithms)\n",
    "    * Creating a Stack of models\n",
    "    * and more ...\n",
    "4. Compare new model to previous model \n",
    "5. Repeat steps 4 and 5 (a reasonable number of times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 1. Identify and Quantify Goal\n",
    "This example is with respect to the Titanic Data Set.\n",
    "\n",
    "**Goal**  \n",
    "Predict whether a passenger would have survived or not, from information about the passenger.  This as a binary classification problem.\n",
    "\n",
    "**Quantify**  \n",
    "Evaluate the \"goodness\" of the model as the percent of predictions that were correctly predicted.  This can be scored with Scikit Learn metric: accuracy_score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "Use EDA to:\n",
    "* get an overview of the data\n",
    "* choose a few simple predictors to start with\n",
    "* document ideas for additional predictors, data transformations, and extracted features to try later\n",
    "\n",
    "This requires a data visualization tool such as Seaborn.  An excellent online course for Seaborn is: [Data Visualization with Seaborn](https://www.datacamp.com/courses/data-visualization-with-seaborn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 3. Create the Initial Model\n",
    "\n",
    "The key point is to get started.  Once something is up an running, it is easy to refine it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4. Improve the Model\n",
    "The goal of each iteration is to create a better model based on what was learned from the previous iteration and what was left untried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 5. Compare Model Performance\n",
    "Progress needs to be measured.  A model needs to be scored and compared to previous models.\n",
    "\n",
    "This topic is more subtle than it may seem.  As such, an entire section is provided below.\n",
    "\n",
    "For this series of notebooks:\n",
    "1. Model Evaluation\n",
    "   * an absolute score used to estimate performance on unseen data\n",
    "   * use 10-Repeated 5 or 10 Fold CV\n",
    "2. Model Selection\n",
    "   * a relative score used to rank models by performance\n",
    "   * use 10-Repeated 2-Fold CV\n",
    "3. Score Significance\n",
    "   * CV scores are \"noisy\".  Decide if one score is better than another, above the noise level, by comparing their confidence intervals.\n",
    "\n",
    "The above is described in more detail in the following section on Model Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 6. Iterate\n",
    "Repeat steps 4 and 5 above until the desired performance is reached or no additional performance can be obtained with the available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Selection<br/>Ranking Models by Score\n",
    "This section presents a deeper look into how to evaluate and compare model performance.\n",
    "\n",
    "This section is more advanced and could be skipped.\n",
    "\n",
    "The primary references used for this section are:\n",
    "1. [Model Evaluation, Model Selection, and Algorithm\n",
    "Selection in Machine Learning by Sebastian Raschka](https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf)\n",
    "2. [Model Selection by David Schonleber](https://towardsdatascience.com/a-short-introduction-to-model-selection-bb1bb9c73376)\n",
    "3. [Cross Validation for Selecting a Model Selection Procedure by Yongli Zhang and Yuhong Yang](http://users.stat.umn.edu/~yangx374/papers/ACV_v30.pdf), in particular, Section 7: \"Misconceptions on the use of CV\".\n",
    "4. [On Over-fiting in Model Selection and Subsequent Selection Bias in Performance Evaluation by Cawley and Talbot](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n",
    "\n",
    "The discussion that follows is simpler than the above and oriented towards small data sets.\n",
    "\n",
    "**Hold Out Set**  \n",
    "If there is sufficient data, it is best to keep a final hold-out set (aka a test set) to score the model on after all experimentation is complete.\n",
    "\n",
    "With small amounts of data, this is not possible.  Setting aside a hold-out set would leave either too little data for training the model well, or too little data for scoring the model well.\n",
    "\n",
    "**Bias and Variance**  \n",
    "The terms bias and variance are used here with respect to the estimate of the model's performance (i.e. its score).\n",
    "\n",
    "A model may have a score that is biased low, because it was trained on too little training data.\n",
    "\n",
    "A model may have a score that has a large variance, because it was scored on too little validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Three Reasons for Model Evaluation\n",
    "1. Estimate the performance of the model on unseen data\n",
    "2. Select the best performing hyperparameters for a given algorithm\n",
    "   * out of the set of hyperparameters tried\n",
    "   * AND estimate performance on unseen data\n",
    "3. Select the best performing algorithm\n",
    "   * out of the set of algorithms tried\n",
    "   * out of the set of hyperparameters tried for each algorithm  \n",
    "   * AND estimate performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate Performance on Unseen Data\n",
    "\n",
    "With a small amount of data, Cross Validation can provide a more accurate score than using a train/test split.\n",
    "\n",
    "10 Fold Cross Validation is often recommended.\n",
    "\n",
    "Repeating the 10-Fold Cross Validation is often recommended.  Repeating reduces the variance of the score, but not its bias.\n",
    "\n",
    "Most of the reduction in variance will be achieved by repeating Cross Validation just 10 or 20 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2. Select the Best Hyperparameters AND Evaluate Performance on Unseen Data\n",
    "\n",
    "There are various methods for exploring the hyperparameter space:\n",
    "* GridSearchCV\n",
    "* RandomSearchCV\n",
    "* hand-coded search\n",
    "* Bayesian Hyperparameter tuning using a third-party library\n",
    "\n",
    "However the hyperparameter values are chosen, the resulting models are scored with Cross Validation.\n",
    "\n",
    "There are two parts to the problem:\n",
    "1. rank each of the models to find the best hyperparameter values\n",
    "2. estimate the performance of this model on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "**Ranking Models**  \n",
    "As Cross Validation is to be used to rank the models, the question becomes what is the best value of K to rank the models.\n",
    "\n",
    "Above it was mentioned that K=10 is a good rule of thumb for estimating a model's performance on unseen data.  Therefore it might seem that K=10 would also work best with respect to ranking models, but this is not the case.\n",
    "\n",
    "See [Cross Validation for Selecting a Model Selection Procedure by Yongli Zhang and Yuhong Yang](http://users.stat.umn.edu/~yangx374/papers/ACV_v30.pdf), in particular, Section 7: \"Misconceptions on the use of CV\".\n",
    "\n",
    "Analogy  \n",
    "A stopwatch which is precisely 10% too slow is used to time your run each day.  With such a stopwatch, you can accurately rank your runs across days.  This is similar to using CV with K < 5.\n",
    "\n",
    "A stopwatch which is on average neither slow nor fast, but does not run consistently at the same speed, is used to time your run each day.  With such as stopwatch, you cannot accurately rank your runs across days.  This is similar to using CV with K=10.\n",
    "\n",
    "When a limited amount of data is available for hyperparameter optimization, a decision has to be made as to how much data to train on, and how much data to validate on.\n",
    "\n",
    "K=2 provides 50% of the data for training and 50% for score validation.  \n",
    "K=10 provides 90% of the data for training, and 10% of the data for score validation.\n",
    "\n",
    "At K=2, the model scores may be biased downward due to having too little data to train on.  \n",
    "At K=10, the model scores may have a lot of variance due to having too little data to validate on.\n",
    "\n",
    "Of course, if there is much too little data for training, the model is not viable at all, and comparing scores becomes meaningless.\n",
    "\n",
    "To use a concrete example taken from later notebooks:\n",
    "* The Titanic data set can be well trained on about 600 records using LogisticRegression\n",
    "* About 900 records are available\n",
    "* K=2 implies 450 records for training and 450 for scoring\n",
    "* K=5 implies 720 records for training and 180 records for scoring\n",
    "\n",
    "Here is a plot taken from a later notebook which shows the mean CV score along with its confidence intervals (${\\pm}$ 1 standard deviation), plotted for each value of K.  The two models (green and red) can only be properly ranked at K=2.\n",
    "\n",
    "That is, even though both models have their score pessimistically biased at K=2, the lower variance is what allows them to be distinguished from one another.\n",
    "\n",
    "A brief discussion of confidence intervals is provided in a later section.\n",
    "\n",
    "![ScoreVsK](ScoreVsK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "**Estimating Performance of Best Model**  \n",
    "Part of the model building process for hyperparameter optimization, is the repeated use of the Cross Validation validation folds.  The use of this validation data as part of the model building process, causes the score to be optimistically biased.\n",
    "\n",
    "If there was sufficient data to have a final hold-out test set, then the best hyperparameter that were found could be used to train a model on all the training data and score that on the test data.\n",
    "\n",
    "However if no final hold-out set is possible due to too little data, then [Nested Cross Validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) can be used.\n",
    "\n",
    "In Nested Cross Validation, the process of selecting the best hyperparameters is performed within the Cross Validation loop.  In the inner loop, model comparison is being perform, so low K (usually K=2) is used.  In the outer loop, the performance on the model on unseen data is being computed, so a higher K (usually K=5) is used.\n",
    "\n",
    "Nested Cross Validation is very computationally intensive, but as it is only needed when there is too little data for a final hold-out test set, that may not be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 3. Select the Best Hyperparameter Optimized Algorithm AND Evaluate Performance on Unseen Data\n",
    "\n",
    "In the preceding case, hyperparameter optimization with model evaluation, either Nested CV can be used with no test set, or CV can be used with a single test set.\n",
    "\n",
    "Here, algorithm selection with hyperparameter optimization with model evaluation, either Nested CV can be used with no test set, or CV can be used with a test set per algorithm.\n",
    "\n",
    "Having mutually exclusive test sets, one per algorithm, requires a lot of data.  This case is more likely to be handled with Nested CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Confidence Intervals  \n",
    "\n",
    "This section presents a brief introduction to confidence intervals.  For more detail, see an introductory book on probability and statistics such as [Practical Statistics for Data Scientists](https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential-ebook/dp/B071NVDFD6/).\n",
    "\n",
    "In order to determine if two values are distinguishable from one another, a confidence interval or equivalent is needed.\n",
    "\n",
    "For example, is 3 close to 7?\n",
    "* 3 ${\\pm}$ 1 is not close to 7 ${\\pm}$ 1\n",
    "* 3 ${\\pm}$ 5 is close to 7 ${\\pm}$ 5\n",
    "\n",
    "In the second example above, 3 ${\\pm}$ 5 represents the confidence interval: \\[-2, 8\\].  As 7 is within this confidence interval, the two measurements could be considered to be the same.  That is, random chance alone may explain the observed difference in values.\n",
    "\n",
    "For Cross Validation, there is no statistically precise definition of variance, standard deviation, margin of error, confidence interval and the like.  For an advanced discussion of this topic, see: [No Unbaised Estimator of Variance of K-Fold CV](http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf).\n",
    "\n",
    "Even though there is no statistically precise definition, the values can be computed and have practical use (as long as discussions of p-values and t-tests and the like are avoided).\n",
    "\n",
    "Some ways to compute a confidence interval include: \n",
    "* K-Fold CV: consider each of the K performance estimates to be IID.  The confidence interval is:\n",
    "```[cv_score_mean - cv_score_sd, cv_score_mean + cv_score_sd]```\n",
    "* M-Repeated K-Fold CV: consider the M\\*K performance estimates to be IID.   The confidence interval is:\n",
    "```[cv_score_mean - cv_score_sd, cv_score_mean + cv_score_sd]```\n",
    "* Box Plot: if M\\*K is sufficiently large, use the median as the point estimate and the Interquartile Range as the confidence interval\n",
    "\n",
    "If the distribution of the model scores is normal, then +/-1 standard deviation covers about 68% of the scores.  Interquartile Range (IQR), as is presented visually on a boxplot, covers 50% of the scores, which is roughly the same as using +/- 1 standard deviation.\n",
    "\n",
    "If the score of one model is within the confidence interval of another, they have effectively the same performance.  For two models that have the same performance, another criteria can be used to distinguish between them, such as model complexity, cost to deploy, etc.\n",
    "\n",
    "Scikit Learn v0.21 Note  \n",
    "GridSearchCV now allows the user to specify a user-written function to determine which model is best.  This function could compute both a confidence interval and a measure of model complexity and use both to decide which model is best.  This feature is helpful because GridSeachCV (with refit=True) may be embedded within cross_val_score.\n",
    "\n",
    "Again, the terms confidence interval, standard deviation, variance and the like are being used loosely here.  These terms are only well defined for IID data and resampled data is not IID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
