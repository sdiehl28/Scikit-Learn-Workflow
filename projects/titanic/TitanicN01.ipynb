{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Iterative Model Development<br/>*The ML Model Building Process*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Goals\n",
    "1. Describe Iterative Model Development.\n",
    "2. Describe how to Compare Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Introduction to Iterative Model Development Series\n",
    "\n",
    "These notebooks are designed to be used with Jupyter Lab and the [Table of Contents](https://github.com/jupyterlab/jupyterlab-toc) plugin.  This plugin allows an outline of the notebook to be viewed in the left-hand panel with the ability to jump to any section. \n",
    "\n",
    "This is the first in a series of notebooks about building Machine Learning models with Scikit Learn.\n",
    "* Only Supervised Learning is considered.\n",
    "* Prediction accuracy is emphasized over model interpretability.\n",
    "* Last notebook in series will present a simple, interpretable, and well performing model.\n",
    "\n",
    "The content is intended to cover common questions a beginner may have, after having made an initial attempt to learn Python, Pandas, Scikit Learn and Machine Learning, elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Machine Learning Models\n",
    "\n",
    "A model is a simplified representation of a system or process.\n",
    "\n",
    "A Machine Learning (ML) Model is created by a machine.  The machine being the computer running a learning algorithm.\n",
    "\n",
    "A ML Model is often a representation of a data generation process.  The Model can be used to classify or predict new data, or understand existing data.\n",
    "\n",
    "Creating a ML Model is a semi-automated data driven process.  A reasonable model can often be created without knowing much about the domain of the data.  A better model can often be created by extracting features and applying data transformations specific to the domain of the data.\n",
    "\n",
    "A learning algorithm \"learns\" or \"fits\" the data it is presented with to create a model.  A model represents data in a manner specific to the learning algorithm that created it.  For example a Linear Regression model represents data as a line.  A Decision Tree Classifier, represents data as a hierarchical set of if-then-else rules.\n",
    "\n",
    "Models with simple internal data representations, such as Linear Regression and Decision Trees, can have their internal representation examined by an end-user to provide insight into the user's data.  Models which have complex internal representations are used to make the best predictions possible.\n",
    "\n",
    "The fitting process is performed by an algorithm which optimizes the model's internal representation of the data.  In Scikit Learn terminology, fitting the data to create a model is called \"minimizing the objective function\". The objective function is a measure of the \"distance\" between the model's representation of the data and the actual data. When this distance has been minimized, a model has been created which has \"learned\" or \"fit\" the data.\n",
    "\n",
    "In general, the measure of distance used by a learning algorithm to best fit a model to the data, is not the same as the measure of distance used by an end-user to evaluate the usefulness of that model for their particular application.  In Scikit Learn terminology, the \"objective function\" used by the learning algorithm is usually different than the \"scoring\" function selected by the end-user.\n",
    "\n",
    "The scoring function should be applied to data that was not used to build the model. The practical value of a predictive model depends upon how well it can predict on new data.\n",
    "\n",
    "In Scikit Learn terminology:\n",
    "1. An **estimator** is a Python object which:\n",
    "   * has an algorithm to fit data\n",
    "   * has internal data structures to hold the results of having fit the data\n",
    "2. **estimator.fit()** fits the data (it creates the model)\n",
    "3. **estimator.predict()** uses the model to make new predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## The Model Building Process\n",
    "This series of notebooks is about how to build ML models in general.  For concreteness, the Titanic data set from Kaggle will be used.\n",
    "\n",
    "Some of the model building techniques presented in the later notebooks of this series are overkill for a data set as simple as the Titanic. However they are likely to be useful for larger and more complex data sets. \n",
    "\n",
    "Model Building Process:  \n",
    "1. Identify and quantify goal\n",
    "2. Perform Exploratory Data Analysis (EDA)\n",
    "3. Quickly build first model and evaluate its usefulness\n",
    "4. Create a better model using better:\n",
    "    * Exploratory Data Analysis \n",
    "    * Preprocessing (data transformations)\n",
    "    * Feature Extraction\n",
    "    * Tuning of hyperparameters\n",
    "    * Other estimators (other model building algorithms)\n",
    "    * Creating a Stack of models\n",
    "    * and more ...\n",
    "4. Compare new model to previous model \n",
    "5. Repeat steps 4 and 5 (a reasonable number of times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 1. Identify and Quantify Goal\n",
    "This example is with respect to the Titanic Data Set.\n",
    "\n",
    "**Goal**  \n",
    "Predict whether a passenger would have survived or not, from information about the passenger.  This as a binary classification problem.\n",
    "\n",
    "**Quantify**  \n",
    "Evaluate the \"goodness\" of the model as the percent of predictions that were correctly predicted.  This can be scored with Scikit Learn metric: accuracy_score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "Use EDA to:\n",
    "* get an overview of the data\n",
    "* choose a few simple predictors to start with\n",
    "* document ideas for additional predictors, data transformations, and extracted features to try later\n",
    "\n",
    "This requires a data visualization tool such as Seaborn.  An excellent online course for Seaborn is: [Data Visualization with Seaborn](https://www.datacamp.com/courses/data-visualization-with-seaborn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 3. Create the Initial Model\n",
    "\n",
    "The key point is to get started.  Once something is up an running, it is easy to refine it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 4. Improve the Model\n",
    "The goal of each iteration is to create a better model based on what was learned from the previous iteration and what was left untried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 5. Compare Model Performance\n",
    "In order to understand if progress is being made, it is necessary to score the model and compare that score to previous models.\n",
    "\n",
    "This topic is more subtle than it may seem.  As such, an entire section is provided below.\n",
    "\n",
    "For this series of notebooks:\n",
    "1. Model Evaluation\n",
    "   * an absolute score used to estimate performance on unseen data\n",
    "   * use 10-Repeated 5 or 10 Fold CV\n",
    "2. Model Selection\n",
    "   * a relative score used to rank models by performance\n",
    "   * use 10-Repeated 2-Fold CV\n",
    "3. Score Significance\n",
    "   * CV scores are \"noisy\".  Decide if one score is better than another, above the noise level, by comparing their confidence intervals.\n",
    "\n",
    "The above is described in more detail in the section on Model Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 6. Iterate\n",
    "Repeat steps 4 and 5 above until the desired performance is reached or no additional performance can be obtained with the available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Selection<br/>Ranking Models by Score\n",
    "This section presents a deeper look into how to evaluate and compare model performance.\n",
    "\n",
    "This section is more advanced and can be skipped.\n",
    "\n",
    "The primary references used for this section are:\n",
    "1. [Model Evaluation, Model Selection, and Algorithm\n",
    "Selection in Machine Learning by Sebastian Raschka](https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf)\n",
    "2. [Model Selection by David Schonleber](https://towardsdatascience.com/a-short-introduction-to-model-selection-bb1bb9c73376)\n",
    "3. [Cross Validation for Selecting a Model Selection Procedure by Yongli Zhang and Yuhong Yang](http://users.stat.umn.edu/~yangx374/papers/ACV_v30.pdf), in particular, Section 7: \"Misconceptions on the use of CV\".\n",
    "4. [On Over-fiting in Model Selection and Subsequent Selection Bias in Performance Evaluation by Cawley and Talbot](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n",
    "\n",
    "The discussion in this notebook is simpler than the above and oriented towards small data sets.\n",
    "\n",
    "**Hold Out Set**  \n",
    "If there is sufficient data, it is best to keep a final hold-out set (aka test set) to score the model on after all experimentation is complete.\n",
    "\n",
    "With small amounts of data, this is not possible.  Setting aside a hold-out set would leave either too little data for training the model well, or too little data for scoring the model well.\n",
    "\n",
    "**Bias and Variance**  \n",
    "The terms bias and variance are used here with respect to the estimate of the model's performance (i.e. its score).\n",
    "\n",
    "A model may have a score that is biased low, because it was trained on too little training data.\n",
    "\n",
    "A model may have a score that has a large variance, because it was scored on too little validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Three Reasons for Model Evaluation\n",
    "1. Estimate the performance of the model on unseen data\n",
    "2. Select the best performing hyperparameters for a given algorithm\n",
    "   * out of the set of hyperparameters tried\n",
    "   * AND estimate performance on unseen data\n",
    "3. Select the best performing algorithm\n",
    "   * out of the set of algorithms tried\n",
    "   * out of the set of hyperparameters tried for each algorithm  \n",
    "   * AND estimate performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate Performance on Unseen Data\n",
    "\n",
    "As suggested above, with a small amount of data, Cross Validation can provide a better score than using a train/test split.\n",
    "\n",
    "This discussion applies to using an out-of-the box SciKit Learn estimator with default hyperparameter values.\n",
    "\n",
    "10 Fold Cross Validation is often recommended.\n",
    "\n",
    "Repeating the 10-Fold Cross Validation is often recommended.  Repeating reduces the variance of the score, but not its bias.\n",
    "\n",
    "Repeating perhaps 10 or 20 times is usually sufficient to reduce the variance about as much as is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2. Select the Best Hyperparameters AND Evaluate Performance on Unseen Data\n",
    "\n",
    "There are various methods for exploring the hyperparameter space:\n",
    "* GridSearchCV\n",
    "* RandomSearchCV\n",
    "* hand-coded search\n",
    "* Bayesian Hyperparameter tuning using a third-party library\n",
    "\n",
    "However the hyperparameter values are chosen, the resulting models are scored with Cross Validation.\n",
    "\n",
    "There are two parts to the problem:\n",
    "1. rank each of the models to find the best hyperparameter values\n",
    "2. estimate the performance of this model on unseen data\n",
    "\n",
    "**Ranking Models**  \n",
    "The question is, what is the best value of K to produce the best ranking of the models?\n",
    "\n",
    "Above it was mentioned that K=10 is a good rule of thumb for estimating a model's performance on unseen data.  It might seem that K=10 would also work best with respect to ranking models, but this is not the case.\n",
    "\n",
    "An analogy may help to make this clear.  If you had a stopwatch that was precisely 10% too slow, you could correctly rank how fast you ran a mile each day.  On the other hand, if you had a stopwatch that was on average neither slow nor fast, but did not run consistently at the same speed, you could not correctly rank how fast you ran each day.  The ability to properly rank how well you ran depends upon both the variance and the bias of the score.\n",
    "\n",
    "When a model is trained on too little data, it has a score that is biased downwards.  If two models are trained on too little data, they both have scores that are biased downward.  To compare the models, the difference in their scores is taken.  If both have a downward bias, the biases largely cancel out.\n",
    "\n",
    "The amount of data used for hyperparameter optimization is usually limited either by computational resources or by simply having too little data.  When using K-Fold Cross Validation, a decision has to be made as to how much data to train on, and how much data to validate on.\n",
    "\n",
    "K=2 provides 50% of the data for training and 50% for score validation.  \n",
    "K=10 provides 90% of the data for training, and 10% of the data for score validation.\n",
    "\n",
    "At K=2, the model scores may be biased downward, but part of this downward bias will cancel out when the scores are subtracted from one another.\n",
    "\n",
    "A K=10, the model scores will have more variance than at K=2 and this variance may obscure which model is best.\n",
    "\n",
    "Of course, if there is much too little data for training, the model is not viable at all, and comparing scores becomes meaningless.\n",
    "\n",
    "A [Learning Curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html) can be used to determine how close to capacity a model is with a given number of records for training.\n",
    "\n",
    "To use a concrete example taken from later notebooks:\n",
    "* The Titanic data set can be well trained on about 600 records using LogisticRegression\n",
    "* About 900 records are available\n",
    "* K=2 implies 450 records for training and 450 for scoring\n",
    "* K=10 implies 810 records for training and 90 records for scoring\n",
    "\n",
    "In the above, K=2 will likely produce a better ranking of models than K=10.\n",
    "\n",
    "Although the scores using 450 records will be biased downward as the model capacity has not yet been reached, the bias will largely cancel out when the difference in scores is taken.  The reduction in variance between scoring on 450 records vs scoring on 90 records will allow K=2 to produce a better ranking than K=10.\n",
    "\n",
    "**Estimating Performance of Best Model**  \n",
    "Built into the model building process for hyperparameter optimization, is the repeated use of the Cross Validation validation folds.  The use of this validation data as part of the model building process, means the score is optimistically biased.\n",
    "\n",
    "If there was sufficient data to have a final hold-out test set, then the best hyperparameter that were found could be used to train a model on all the training data and score that on the test data.\n",
    "\n",
    "However if no final hold-out set is possible due to too little data, then [Nested Cross Validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) can be used.\n",
    "\n",
    "In Nested Cross Validation, the process of selecting the best hyperparameters is performed within the Cross Validation loop.  In the inner loop, model comparison is being perform, so low K (usually K=2) is used.  In the outer loop, the performance on the model on unseen data is being computed, so a higher K (usually K=5) is used.  I suspect K=5 is used in this situation because it reduces the computation time relative to K=10 and yet produces very similar results.\n",
    "\n",
    "Nested Cross Validation is very computationally intensive, but as it is only needed when there is too little data for a final hold-out test set, that may not be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 3. Select the Best Hyperparameter Optimized Algorithm AND Evaluate Performance on Unseen Data\n",
    "\n",
    "The key point about having a final hold-out test set for evaluation is that it can only be used once (or very few times).\n",
    "\n",
    "If several different learning algorithms are tried, and each is evaluated on the same test set, then the test set has been used multiple times and it no longer provides an unbiased estimate of model performance.\n",
    "\n",
    "If there was a very large amount of data, than there could be many final mutually exclusive hold-out test sets, one for each algorithm, and this would be fine.  However this requires a very large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Confidence Intervals  \n",
    "Is 3 close to 7?\n",
    "* 3 ${\\pm}$ 1 is not close to 7 ${\\pm}$ 1\n",
    "* 3 ${\\pm}$ 5 is close to 7 ${\\pm}$ 5\n",
    "\n",
    "In order to determine if two values are distinguishable from one another, a confidence interval or equivalent is needed.\n",
    "\n",
    "In the second example above, 3 ${\\pm}$ 5 represents the confidence interval: \\[-2, 8\\].  As 7 is within this confidence interval, the two measurements could be considered to be the same.  That is, random chance alone may explain the observed difference in values.\n",
    "\n",
    "For more detail, see an introductory book on probability and statistics.  A good book for Data Sciencties is: [Practical Statistics for Data Scientists](https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential-ebook/dp/B071NVDFD6/)\n",
    "\n",
    "For Cross Validation, there is no statistically precise definition of variance, standard deviation, margin of error, confidence interval and the like.  Nevertheless, these values can be computed and are useful.  For an advanced discussion, see: [No Unbaised Estimator of Variance of K-Fold CV](http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf)\n",
    "\n",
    "Some ways to compute a confidence interval include: \n",
    "* K-Fold CV: consider each of the K performance estimates to be IID.  The confidence interval is:\n",
    "```[cv_score_mean - cv_score_sd, cv_score_mean + cv_score_sd]```\n",
    "* M-Repeated K-Fold CV: consider the M\\*K performance estimates to be IID.   The confidence interval is:\n",
    "```[cv_score_mean - cv_score_sd, cv_score_mean + cv_score_sd]```\n",
    "* Alternative: if M\\*K is sufficiently large, use the median as the point estimate and the Interquartile Range as the confidence interval\n",
    "\n",
    "If the distribution of the model scores is normal, then +/-1 standard deviation covers about 68% of the scores.  Interquartile Range (IQR), as is presented visually on a boxplot, covers 50% of the scores, so the box in a boxplot is similar to a confidence interval defined as the mean +/- 1 standard deviation.\n",
    "\n",
    "If the score of one model is within the confidence interval of another, they have effectively the same performance.  This means some other criteria can be used to distinguish between them, such as complexity, cost to deploy, etc.  Note that the latest version of GridSearchCV allows the user to specify a function which determines which model is best using any user-defined criteria.\n",
    "\n",
    "Again, the terms confidence interval, standard deviation, variance and the like are being used loosely here.  These terms are only well defined for IID data and resampled data is not IID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
