{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Cross Validation<br/>*Avoiding Data Leakage*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "1. Standardize the variables\n",
    "2. Demonstrate the difference between standardizing all the data up front vs standardizing the data on training data only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Avoiding Test Data Leakage\n",
    "There are two common statements made about model building and the test set.  The second statement includes the first.\n",
    "1. Never use the test set's target variable as part of the model building process.\n",
    "2. Never use any of the test set data as part of the model building process.\n",
    "\n",
    "The first statement is always true<sup>[1](#footnotes)</sup>.  \n",
    "The second statement is good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Avoiding use of test data  \n",
    "```python\n",
    "s = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y)\n",
    "X_train_transform = s.fit_transform(X_train)\n",
    "X_test_transform = s.fit(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:  Using non target values of the test data  \n",
    "```python\n",
    "s = StandardScaler()\n",
    "\n",
    "X_transform = s.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_transform, y)\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a medium sized homogeneous data set, the two examples above will likely result in nearly the same model being created.  This is because random samples of len(X_train) will have similar mean and standard deviations.  However for small amounts of data, or for unbalanced data, the second example above will produce a model that is overly optimistic in its ability to predict on unseen data.\n",
    "\n",
    "When using cross validation instead of a train/test split, the easiest way to perform the transform without using the test data is to use a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  \n",
    "\n",
    "Below Standardization will be performed both with and without looking at the test data in order to show the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"footnotes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Footnotes\n",
    "1. \"On Comparison of feature selection algorithms\" \\[Refaeilzadeh et al., 2007\\] states that for *ranking* feature selection algorithms for small amounts of data, it may be permissible to look at the target variable in a limited sense.  Unless you are an expert though, the target variable in the test set should not be used as part of the model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style\n",
    "\n",
    "import titanic_helper_code as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:      3.7.3 (default, Mar 27 2019, 22:11:17) \n",
      "[GCC 7.3.0]\n",
      "numpy:       1.16.4\n",
      "pandas:      0.24.2\n",
      "matplotlib:  3.1.0\n",
      "seaborn:     0.9.0\n",
      "sklearn:     0.21.2\n",
      "Description:\tUbuntu 18.04.2 LTS\n"
     ]
    }
   ],
   "source": [
    "# Version Information\n",
    "import sys\n",
    "print('python:     ', sys.version)\n",
    "print('numpy:      ', np.__version__)\n",
    "print('pandas:     ', pd.__version__)\n",
    "import matplotlib\n",
    "print('matplotlib: ', matplotlib.__version__)\n",
    "print('seaborn:    ', sns.__version__)\n",
    "print('sklearn:    ', sk.__version__)\n",
    "!lsb_release -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Model Building Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from my titanic_helper_code.py\n",
    "def get_Xy_v1(filename='./data/train.csv'):\n",
    "    \"\"\"Data Encoding for Iteration 1\n",
    "\n",
    "    Version 1\n",
    "    * Pclass, Fare, and Sex encoded as 1/0 for female/male\n",
    "    \"\"\"\n",
    "\n",
    "    # read data\n",
    "    all_data = pd.read_csv(filename)\n",
    "    X = all_data.drop('Survived', axis=1)\n",
    "    y = all_data['Survived']\n",
    "    \n",
    "    # encode data\n",
    "    X['Sex'] = X['Sex'].replace({'female':1, 'male':0})\n",
    "    \n",
    "    # drop unused columns\n",
    "    drop_columns = ['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', \n",
    "                    'Ticket', 'Cabin', 'Embarked']\n",
    "    X = X.drop(drop_columns, axis=1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tt.get_Xy_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"crossvalidation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: Without Use of Test Data\n",
    "The goal is simply to show there is a difference between computing the accuracy when no test data is looked at vs when some of the test data is looked at (but not the target variable).\n",
    "\n",
    "In order to show a difference in what follows, strong regularization is used, C=0.001.\n",
    "\n",
    "In the next notebook, a Pipeline will be used to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a specific set of CV folds for repeatability\n",
    "cv_select = RepeatedStratifiedKFold(n_splits=2, n_repeats=10, \n",
    "                                    random_state=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Scores  min:0.738 max:0.809\n",
      "CV Mean Score: 0.784 +/- 0.016\n"
     ]
    }
   ],
   "source": [
    "# perform CV with transformation, without a pipe, \n",
    "# to illustrate the concept\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "lr = LogisticRegression(penalty='l2', C=0.001, solver='liblinear')\n",
    "\n",
    "score_per_fold = []\n",
    "for train_idx, test_idx in cv_select.split(X,y):\n",
    "    \n",
    "    # train subset\n",
    "    X_train = X.iloc[train_idx, :]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    \n",
    "    # test subset\n",
    "    X_test = X.iloc[test_idx, :]\n",
    "    y_test = y.iloc[test_idx]\n",
    "    \n",
    "    # standardize the variables on train\n",
    "    X_train_transformed = ss.fit_transform(X_train)\n",
    "    \n",
    "    # fit model on train\n",
    "    lr.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # standardize variables on test\n",
    "    X_test_transformed = ss.transform(X_test) # not fit_transform!\n",
    "    \n",
    "    # predict using fitted model on test\n",
    "    predictions = lr.predict(X_test_transformed)\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    fold_score = accuracy_score(y_test, predictions)\n",
    "    score_per_fold.append(fold_score)\n",
    "    \n",
    "scores = np.array(score_per_fold)\n",
    "tt.print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: With Use of Test Data\n",
    "A common way but not necessarily good method is to standardize the variables over the *entire* dataset and then estimate model accuracy using either a train/test split or cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  False\n",
      "Scores Diff:   0.0033\n"
     ]
    }
   ],
   "source": [
    "# Prior to Cross Validation: standardize *all* the data up front\n",
    "# This is \"data leakage\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "wrong_scores = cross_val_score(lr, X_scaled, y, \n",
    "                               cv=cv_select, scoring='accuracy')\n",
    "\n",
    "# We do *not* get the same scores as above!\n",
    "print(\"Scores Match: \", (scores == wrong_scores).all())\n",
    "print(\"Scores Diff:  \", np.round(wrong_scores.mean() - scores.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Standardizing all the values prior to evaluating the model with cross validation led to an estimate of model performance that was slightly too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Summary\n",
    "**Best practice is to ensure that the entire model building process is encapsulated inside of cross validation.**  \n",
    "\n",
    "This is most easily accomplished by encapsulating all data transformation operations inside of a pipe, as will be shown in the next notebook.\n",
    "\n",
    "A good story about the above is presented by Robert Tibshirani, in the youtube video [Cross Validation: Right and Wrong](https://www.youtube.com/watch?v=S06JpVoNaA0&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf).  In this case, \"wrong\" means to perform feature selection using the target variable in the test set, outside of the cross validation loop.\n",
    "\n",
    "Note that encoding data is not the same as transforming data.  For example, encoding \"Passenger First Class\" as 1, and \"Passenger Second Class\" as 2, can be performed without looking at the test set at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
