{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Model Building: Iteration 4<br/>*Categorical Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Goals\n",
    "1. Determine which string variables may be helpful\n",
    "2. Encode these variables\n",
    "3. Evaluate and compare with previous model run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding and Avoiding Data Leakage\n",
    "Encoding of Categorical variables can be performed on the entire data set up front.\n",
    "\n",
    "Domain knowledge usually determines what distinct values a particular categorical variable can have. This knowledge is independent of any test set, so data leakage is (usually) not a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding\n",
    "There are three primary questions to ask:\n",
    "1. Are the categorical values ordered?\n",
    "2. Are there a large number of distinct values?\n",
    "3. Are all values known in advance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Ordered\n",
    "If the categorical value has an inherent ordering, then converting the sorted values to consecutive integers is a good approach.  The Titanic data came with this encoding for 1st-Class, 2nd-Class, and 3rd-Class already performed.\n",
    "\n",
    "Some models, such as Linear and Logistic Regression, inherently make use of the concept of distance.  That is, the difference between 1st and 2nd class will be assumed by the model to be the same as the difference between 2nd and 3rd class.  Nonlinear classifiers, such as DecisionTrees and Forests, will not make this assumption.\n",
    "\n",
    "Assuming equi-spaced values are equi-spaced in the problem domain may be the best assumption that can be made in the absence of domain information.  With the Titanic data, knowing that there were extra physical barriers that third class passengers had to overcome to reach the lifeboats, suggests that encoding third class as a 4 rather than a 3, might be better for a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Unordered: One Hot Encoding\n",
    "A categorical variable with n distinct values will be converted to n dummy variables.  For each row, exactly one dummy variable will be 1 and the rest will be zero.\n",
    "\n",
    "For example, \"Male\", \"Female\" might be encoded as two dummy variables, is_female and is_male, as follows: \n",
    "* \"Female\" -> is_female = 1, is_male = 0\n",
    "* \"Male\" -> is_female = 0, is_male = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "Some estimators, such as Linear Regression and Logistic Regression, have unstable solutions if their input variables are linear combinations of one another.  These solutions become stable if regularization is used.\n",
    "\n",
    "By default, Scikit Learn's LogisticRegression uses regularization but its LinearRegression estimator does not.  In Scikit Learn, the regularized versions of LinearRegression are the Lasso, Ridge, and ElasticNet.\n",
    "\n",
    "Encoding a categorical variable with n levels into n dummy variables means one variable is a linear combination of the others.  This is easiest to see when there are two levels.  If Sex can only have two mutually exclusive values, and is encoded as two dummy variables, such as is_male and is_female, then knowing one means knowing the other.  Instead, Sex can be encoded into a single variable, such as is_female.\n",
    "\n",
    "ScitKit Learn's OneHotEncoder, as of May 2019 v0.21, now allows for reducing the number of dummy variables generated by one.  This is important if LinearRegression is to be used as the estimator (or LogisticRegression is used without regularization).\n",
    "\n",
    "Pandas offers a method similar to OneHotEncoder, pd.get_dummies(), which also allows for the option of creating one less dummy variable to avoid multicollinearity.\n",
    "\n",
    "Here, LogisticRegression is being used *with regularization*, so it is optional whether or not to include the extra dummy variable.  Depending on how you interpret the model, having the extra dummy variable will make it either more, or less, interpretable.  Below, the extra dummy variable is included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Unordered: Large Number of Distinct Values\n",
    "For each distinct value, a dummy variable will be created.  If there are a large number of distinct values, many dummy variables will be created.  The more dummy variables, the higher the dimension and the more likely that \"the curse of dimensionality\" becomes a problem.  Some estimators, such as KNN, do not work well with a high number of dimensions.\n",
    "\n",
    "One option is to exclude a categorical variable with a large number of distinct values from the model.\n",
    "\n",
    "Another option is to reduce the number of distinct values. Some ways this could be done include:\n",
    "* have a domain expert re-categorize the values into a smaller number of categories\n",
    "* automatically map all rare values into a single category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical: All Values not Known Up Front\n",
    "One solution is to encode all values never seen before, into a single value or single dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style\n",
    "\n",
    "import titanic_helper_code as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:      3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "numpy:       1.17.4\n",
      "pandas:      0.25.3\n",
      "matplotlib:  3.1.3\n",
      "seaborn:     0.9.0\n",
      "sklearn:     0.22\n",
      "Description:\tUbuntu 18.04.4 LTS\r\n"
     ]
    }
   ],
   "source": [
    "# Version Information\n",
    "import sys\n",
    "print('python:     ', sys.version)\n",
    "print('numpy:      ', np.__version__)\n",
    "print('pandas:     ', pd.__version__)\n",
    "import matplotlib\n",
    "print('matplotlib: ', matplotlib.__version__)\n",
    "print('seaborn:    ', sns.__version__)\n",
    "print('sklearn:    ', sk.__version__)\n",
    "!lsb_release -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explictly enable and import new iterative imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (891, 11)\n",
      "y Shape:  (891,)\n"
     ]
    }
   ],
   "source": [
    "# read in Titanic data\n",
    "all_data = pd.read_csv('./data/train.csv')\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "print('X Shape: ', X.shape)\n",
    "print('y Shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  891\n",
      "\n",
      "Number of Distinct Values\n",
      "PassengerId: 891\n",
      "Name:        891\n",
      "Ticket:      681\n",
      "Cabin:       147\n",
      "Embarked:      3\n"
     ]
    }
   ],
   "source": [
    "print('Number of records: ', len(X))\n",
    "print('\\nNumber of Distinct Values')\n",
    "print(f'PassengerId: {X[\"PassengerId\"].nunique():3d}')\n",
    "print(f'Name:        {X[\"Name\"].nunique():3d}')      \n",
    "print(f'Ticket:      {X[\"Ticket\"].nunique():3d}')\n",
    "print(f'Cabin:       {X[\"Cabin\"].nunique():3d}')      \n",
    "print(f'Embarked:    {X[\"Embarked\"].nunique():3d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Every PassengerId is unique, so this carries no information.\n",
    "* Every Name is unique, so this carries no information (as is).\n",
    "* Most Ticket values are unique, so this carries very little information.\n",
    "\n",
    "PassengerId, Name, and Ticket will not be used as is.\n",
    "\n",
    "As per the EDA notebook, title can be extracted from the Name field.\n",
    "\n",
    "title and Embarked will be encoded as dummy variables and added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['Embarked'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two null records, given that 3 dummy variables will be generated, will be encoded as all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From EDA notebook\n",
    "def extract_title(x):\n",
    "    title = x.split(',')[1].split('.')[0].strip()\n",
    "    if title not in ['Mr', 'Miss', 'Mrs', 'Master']:\n",
    "        title = 'Other'\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr        517\n",
       "Miss      182\n",
       "Mrs       125\n",
       "Master     40\n",
       "Other      27\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['title'] = X['Name'].apply(extract_title)\n",
    "X['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embarked has 3 distinct values\n",
    "X['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding title and Embarked into Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three lines of code are required to encode Embarked and title as dummy variables.  The rest of this code is from previous notebooks.\n",
    "\n",
    "Null values are encoded as all dummy variables having the value of 0.  There are two records with null Embarked values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from titanic_helper_code.py\n",
    "def get_Xy_v4(filename='./data/train.csv'):\n",
    "    \"\"\"Data Encoding\n",
    "\n",
    "    Version 4\n",
    "    * Pclass and Sex encoded as 1/0\n",
    "    * Age, Fare, SibSp, Parch\n",
    "    * family_size, is_cabin_notnull, is_large_family\n",
    "    * is_child, is_boy, is_sibsp_zero, is_parch_zero\n",
    "    * extract Title and dummy encode it\n",
    "    * dummy encode Embarked\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_title(x):\n",
    "        title = x.split(',')[1].split('.')[0].strip()\n",
    "        if title not in ['Mr', 'Miss', 'Mrs', 'Master']:\n",
    "            title = 'Other'\n",
    "        return title\n",
    "    \n",
    "    # read data\n",
    "    all_data = pd.read_csv('./data/train.csv')\n",
    "    X = all_data.drop('Survived', axis=1)\n",
    "    y = all_data['Survived']\n",
    "\n",
    "    # encode data\n",
    "    X['Sex'] = X['Sex'].replace({'female': 1, 'male': 0})\n",
    "    X['family_size'] = X['SibSp'] + X['Parch'] + 1\n",
    "    X['is_cabin_notnull'] = X['Cabin'].notnull()\n",
    "    X['is_large_family'] = (X['family_size'] > 4)\n",
    "    X['is_sibsp_zero'] = (X['SibSp'] == 0)\n",
    "    X['is_parch_zero'] = (X['Parch'] == 0)\n",
    "\n",
    "    # comparison with null is false\n",
    "    # so is_child and is_boy are false when age is null\n",
    "    X['is_child'] = (X['Age'] < 18)\n",
    "    X['is_boy'] = (X['Age'] < 18) & (X['Sex'] == 0)\n",
    "\n",
    "    # dummy encode title and Embarked\n",
    "    title = X['Name'].apply(extract_title)\n",
    "    dummy_title = pd.get_dummies(title, prefix='Title')\n",
    "    dummy_embarked = pd.get_dummies(X['Embarked'], prefix='Port')\n",
    "    X = pd.concat([X, dummy_embarked, dummy_title], axis=1)\n",
    "\n",
    "    # drop unused columns\n",
    "    drop_columns = ['PassengerId', 'Name',\n",
    "                    'Ticket', 'Embarked', 'Cabin']\n",
    "    X = X.drop(drop_columns, axis=1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'family_size',\n",
       " 'is_cabin_notnull',\n",
       " 'is_large_family',\n",
       " 'is_sibsp_zero',\n",
       " 'is_parch_zero',\n",
       " 'is_child',\n",
       " 'is_boy',\n",
       " 'Port_C',\n",
       " 'Port_Q',\n",
       " 'Port_S',\n",
       " 'Title_Master',\n",
       " 'Title_Miss',\n",
       " 'Title_Mr',\n",
       " 'Title_Mrs',\n",
       " 'Title_Other']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = tt.get_Xy_v4()\n",
    "X.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from titanic_helper_code.py\n",
    "def get_ct_v3():\n",
    "    \"\"\"Column Transform for Features\n",
    "\n",
    "    Version 3\n",
    "    * with Categorical Variable Encoding\n",
    "    * uses all columns for Wrapped IterativeImputer\n",
    "\n",
    "    Returns column names and ColumnTransform instance.\n",
    "    \"\"\"\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ii = WrappedIterativeImputer('Age')\n",
    "    kbin = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='quantile')\n",
    "\n",
    "    # Pipelines\n",
    "    ss_pipe = Pipeline([('ss', ss)])\n",
    "    ii_ss_pipe = Pipeline([('ii', ii), ('ss', ss)])\n",
    "    kbin_pipe = Pipeline([('kbin', kbin)])\n",
    "\n",
    "    # Columns to act on\n",
    "    ss_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'family_size']\n",
    "    ii_ss_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'family_size',\n",
    "                  'is_cabin_notnull', 'is_large_family', 'is_child', 'is_sibsp_zero',\n",
    "                  'is_parch_zero', 'is_boy', 'Port_C', 'Port_Q', 'Port_S', \n",
    "                  'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "    kbin_cols = ['Fare']\n",
    "    bool_cols = ['Sex', 'is_cabin_notnull', 'is_large_family', 'is_child', \n",
    "                 'is_sibsp_zero', 'is_parch_zero', 'is_boy',\n",
    "                 'Port_C', 'Port_Q', 'Port_S', 'Title_Master',\n",
    "                 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "\n",
    "    transformers = [('ss_tr', ss_pipe, ss_cols),\n",
    "                    ('ii_ss_tr', ii_ss_pipe, ii_ss_cols),\n",
    "                    ('kbin_tr', kbin_pipe, kbin_cols),\n",
    "                    ('as_is', 'passthrough', bool_cols)]\n",
    "\n",
    "    ct = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # there is no way to access the columns by name from a pipe\n",
    "    # create a list of columns to keep track\n",
    "    cols = ss_cols + ['Age'] + ['is_fare_high'] + bool_cols\n",
    "\n",
    "    return cols, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for model comparison, not model selection\n",
    "cv_select = RepeatedStratifiedKFold(n_splits=2, n_repeats=10, \n",
    "                                    random_state=108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Scores  min:0.798 max:0.858\n",
      "CV Mean Score: 0.829 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "X, y = tt.get_Xy_v4()\n",
    "cols, ct = tt.get_ct_v3()\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "pipe = Pipeline([('ct', ct), ('lr', lr)])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=cv_select, scoring='accuracy')\n",
    "tt.print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores have improved by adding Embarked and title.\n",
    "\n",
    "At this point, Age is being imputed from all the variables, however Pclass, Sex and Title have the most direct relationship to age (this analysis is not shown here).  In the next ColumnTransformer, use only these variables for Age imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from titanic_helper_code.py\n",
    "def get_ct_v4():\n",
    "    \"\"\"Column Transform for Features\n",
    "\n",
    "    Version 4\n",
    "    * with Categorical Variable Encoding\n",
    "    * use subset of variables for Wrapped IterativeImputer\n",
    "\n",
    "    Returns column names and ColumnTransform instance.\n",
    "    \"\"\"\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ii = WrappedIterativeImputer('Age')\n",
    "    kbin = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='quantile')\n",
    "\n",
    "    # Pipelines\n",
    "    ss_pipe = Pipeline([('ss', ss)])\n",
    "    ii_ss_pipe = Pipeline([('ii', ii), ('ss', ss)])\n",
    "    kbin_pipe = Pipeline([('kbin', kbin)])\n",
    "\n",
    "    # Columns to act on\n",
    "    ss_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'family_size']\n",
    "    ii_ss_cols = ['Pclass', 'Sex', 'Age', 'Title_Master',\n",
    "                  'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "    kbin_cols = ['Fare']\n",
    "    bool_cols = ['Sex', 'is_cabin_notnull', 'is_large_family', 'is_child',\n",
    "                 'is_sibsp_zero', 'is_parch_zero', 'is_boy',\n",
    "                 'Port_C', 'Port_Q', 'Port_S', 'Title_Master',\n",
    "                 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "\n",
    "    transformers = [('ss_tr', ss_pipe, ss_cols),\n",
    "                    ('ii_ss_tr', ii_ss_pipe, ii_ss_cols),\n",
    "                    ('kbin_tr', kbin_pipe, kbin_cols),\n",
    "                    ('as_is', 'passthrough', bool_cols)]\n",
    "\n",
    "    ct = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # there is no way to access the columns by name from a pipe\n",
    "    # create a list of columns to keep track\n",
    "    cols = ss_cols + ['Age'] + ['is_fare_high'] + bool_cols\n",
    "\n",
    "    return cols, ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Scores  min:0.800 max:0.861\n",
      "CV Mean Score: 0.829 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "X, y = tt.get_Xy_v4()\n",
    "cols, ct = tt.get_ct_v4()\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "pipe = Pipeline([('ct', ct), ('lr', lr)])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=cv_select, scoring='accuracy')\n",
    "tt.print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above may be slightly better.  The only difference was to impute age from a subset of all the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify\n",
    "After adding numerous extracted features, in part because LogisticRegression does not handle non-linearities well, it is useful to take a step back, simplify the preprocessing based on domain knowledge and information content, and see what happens.\n",
    "\n",
    "Predictors\n",
    "* Pclass\n",
    "* Sex\n",
    "* Fare\n",
    "* Age -- use new iterative imputer for missing values\n",
    "* FamilySize -- makes more sense than its component variables (SibSp and Parch)\n",
    "* Cabin -- useful as null/not-null\n",
    "* Name -- useful for extracted Title represented as dummy variables\n",
    "* Embarked -- represent as dummy variables (small loss if removed)\n",
    "\n",
    "It is reasonable to believe the above 8 columns alone carry almost all of the useful information content.  Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from titanic_helper_code.py\n",
    "def get_ct_v5():\n",
    "    \"\"\"Column Transform for Features\n",
    "\n",
    "    Version 5\n",
    "    * with Categorical Variable Encoding\n",
    "    * use subset of variables for Wrapped IterativeImputer\n",
    "    * use subset of variables for prediction\n",
    "\n",
    "    Returns column names and ColumnTransform instance.\n",
    "    \"\"\"\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ii = WrappedIterativeImputer('Age')\n",
    "\n",
    "    # Pipelines\n",
    "    ss_pipe = Pipeline([('ss', ss)])\n",
    "    ii_ss_pipe = Pipeline([('ii', ii), ('ss', ss)])\n",
    "\n",
    "    # Columns to act on\n",
    "    ss_cols = ['Pclass', 'Fare', 'family_size']\n",
    "    ii_ss_cols = ['Pclass', 'Sex', 'Age', 'Title_Master',\n",
    "                  'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "    bool_cols = ['Sex', 'is_cabin_notnull',\n",
    "                 'Port_C', 'Port_Q', 'Port_S', 'Title_Master',\n",
    "                 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other']\n",
    "\n",
    "    transformers = [('ss_tr', ss_pipe, ss_cols),\n",
    "                    ('ii_ss_tr', ii_ss_pipe, ii_ss_cols),\n",
    "                    ('as_is', 'passthrough', bool_cols)]\n",
    "\n",
    "    ct = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # there is no way to access the columns by name from a pipe\n",
    "    # create a list of columns to keep track\n",
    "    cols = ss_cols + ['Age'] + bool_cols\n",
    "\n",
    "    return cols, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Scores  min:0.798 max:0.863\n",
      "CV Mean Score: 0.830 +/- 0.014\n"
     ]
    }
   ],
   "source": [
    "X, y = tt.get_Xy_v4()\n",
    "cols, ct = tt.get_ct_v5()\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "pipe = Pipeline([('ct', ct), ('lr', lr)])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=cv_select, scoring='accuracy')\n",
    "tt.print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpler model looks just as good as the one with the extra extracted features.  Use the simpler one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another minor tweak that can be made.  As per the discussion regarding the Titanic data set for passenger class, 3rd class is not just on a lower deck, but it also had additional physical barriers that a passenger had to overcome to reach the life boats.\n",
    "\n",
    "LinearRegression treats its features as though the concept of distance is defined.  That is, the difference between 1st class and 2nd class is treated as if it is the same as the distance between 2nd class and 3rd class.  Arguably this is not the case given the extra physical barriers 3rd class passengers had to overcome to reach the life boats.\n",
    "\n",
    "Try the following encoding:  \n",
    "1st class -> 1  \n",
    "2nd class -> 2  \n",
    "3rd class -> 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from titanic_helper_code.y\n",
    "def get_Xy_v6(filename='./data/train.csv'):\n",
    "    \"\"\"Data Encoding\n",
    "\n",
    "    Version 5\n",
    "    * same as version 4 except encode 3rd class as the number 4\n",
    "    * to better reflect the added difficultly of being in 3rd class\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_title(x):\n",
    "        title = x.split(',')[1].split('.')[0].strip()\n",
    "        if title not in ['Mr', 'Miss', 'Mrs', 'Master']:\n",
    "            title = 'Other'\n",
    "        return title\n",
    "\n",
    "    # read data\n",
    "    all_data = pd.read_csv('./data/train.csv')\n",
    "    X = all_data.drop('Survived', axis=1)\n",
    "    y = all_data['Survived']\n",
    "\n",
    "    # encode data\n",
    "    X['Sex'] = X['Sex'].replace({'female': 1, 'male': 0})\n",
    "    X['Pclass'] = X['Pclass'].replace({1:1, 2:2, 3:4})\n",
    "    X['family_size'] = X['SibSp'] + X['Parch'] + 1\n",
    "    X['is_cabin_notnull'] = X['Cabin'].notnull()\n",
    "\n",
    "    # dummy encode title and Embarked\n",
    "    title = X['Name'].apply(extract_title)\n",
    "    dummy_title = pd.get_dummies(title, prefix='Title')\n",
    "    dummy_embarked = pd.get_dummies(X['Embarked'], prefix='Port')\n",
    "    X = pd.concat([X, dummy_embarked, dummy_title], axis=1)\n",
    "\n",
    "    # drop unused columns\n",
    "    drop_columns = ['PassengerId', 'Name', 'SibSp', 'Parch',\n",
    "                    'Ticket', 'Embarked', 'Cabin']\n",
    "    X = X.drop(drop_columns, axis=1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Scores  min:0.803 max:0.865\n",
      "CV Mean Score: 0.832 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "X, y = tt.get_Xy_v6()\n",
    "cols, ct = tt.get_ct_v5()\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "pipe = Pipeline([('ct', ct), ('lr', lr)])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=cv_select, scoring='accuracy')\n",
    "tt.print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be slightly better.\n",
    "\n",
    "Note that this encoding only makes a difference with an estimator that considers the distance between equally spaced numeric values to be the same.  This change in encoding would have no effect on a Tree based estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scores for comparison with next iteration\n",
    "np.save(\"./data/iter04.data\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model with Previous Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAEdCAYAAACL5fleAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deVhV1cLH8e9hRnFCccIx9UhqZqam5qygr2aaDZZioXmx0LIyNe/Nl6TpVt40zVfMAUkth9T0pqWZOGYopI9jeHF4FWcUJ5ThwH7/8D0nT4CCYODu93menmqvddZee3M4v7PWXntjMQzDQERExARcirsDIiIiRUWhJiIipqFQExER01CoiYiIaSjURETENBRqIiJiGgo1ERExDYWaiIiYhkJNRERMQ6EmIiKmoVATERHTUKiJiIhpKNRERMQ0FGoiImIabsXdgb+C7GwDmy2ruLshInJPcHNzxcXFcmevLeK+SC5stiwuXbpe3N0QEbknlCvnjYfHncWTph9FRMQ0FGoiImIaCjURETENhZqIiJiGQk1ERExDoSYiIqahJf1yWz/8sIoVK5aSnp5W3F0pETw9vejT50l69OhV3F0RkT/QSE1ua82aVQq0m6Snp7Fmzari7oaI5EKhJrfVvXsvPD29irsbJYanpxfdu2uUJlISWQzDMIq7E2aXkWHTE0WKwJAhAxz/PWfOV8XYExG5m/REERERERRqIiJiIgo1ERExDYWaiIiYhkJNRERMQ6EmIiKmoVATERHTUKiJiIhpKNRERMQ0FGoiImIaCjURETENhZqIiJiGQk1ERExDoSYiIqahUBMREdNQqImIiGko1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaSjURETENBRqIiJiGgo1ERExDYWaiIiYhkJNRERMQ6EmIiKm4VZUDf38889ERkaSkJBAZmYmjRs3JjQ0lPbt2+e7jV27djF9+nR27tzJtWvXqFq1Kl26dGH48OGUK1cuR/1u3bpx/PjxPNvbt28fbm6/H2JaWhrR0dGsXLmSpKQkypQpQ6dOnXj11VepXLlywQ5YRERKnCIJtWXLljFu3Dg8PDxo3bo12dnZxMbGMnToUCIiIujfv/9t21i3bh0jR47EZrPRtGlT/Pz82LNnD9HR0WzYsIGFCxfi6+vrqH/lyhWSkpKoVKkSbdq0ybVNF5ffB6KZmZmEhYWxdetWqlWrRseOHTl8+DBLlixhw4YNLF68mOrVqxf+ZIiISLEpdKidPXuW8PBwypQpw1dffYXVagVg9+7dDB48mPfff59OnTpRpUqVPNuw2WyEh4eTnZ3N1KlTCQoKAiA9PZ2RI0cSExPDtGnTGD9+vOM1Bw4cwDAMOnfuzHvvvXfbfs6fP5+tW7fSqVMnpk6dioeHBwCTJk0iMjKSiIgIIiMjC3MqRESkmBX6mtr8+fPJyMggJCTEEWgATZs2ZejQoaSnp7No0aJbtpGQkEBycjIBAQGOQAPw9PQkLCwMgB07dji9Zv/+/QA0btz4tn00DIOoqCgsFgvjx493BBrAyJEjqVu3LjExMbecyhQRkZKv0KG2efNm4Mb1rT8KDAwEYNOmTbfuxP9PE54/fx6bzeZUlpKSApDjmtqBAweA/IVaQkICZ86cISAggBo1auTYd5cuXfLVTxERKdkKFWqGYZCYmIiLiwv33XdfjvI6derg4uJCYmIihmHk2U79+vWpVq0aZ86cYcyYMRw7dozr16+zbds2JkyYgIuLC4MHD3Z6zf79+3F1deXIkSMMHDiQFi1a0LJlS1566SV2797tVDcxMRGABg0a5Lp/e98PHjxYoOMXEZGSpVDX1C5dukRGRga+vr5OU3qOxt3cqFChAufPnyc1NRUfH59c23F3d2fKlCmMGDGCVatWsWrVKkdZ5cqVmTlzJu3atXNsy8jI4PDhw2RlZTFmzBgeeOABHnnkEf7zn/8QExPDli1bmDhxIj169ADg3LlzAPj5+eW6f/v28+fP39mJuA0PDzf8/Mrclbb/qnQ+RSQ3hRqpXb9+HQBvb+8863h5eQGQmpp6y7Zq1apF7969cXV1pWnTpnTu3Bk/Pz/Onj3L7NmzuXjxoqNuQkICNpuN0qVLM3fuXL755humTZvG2rVrGTduHJmZmYwbN84RZteuXbtlP+19tNcTEZF7U6FGajcvmc/LraYd7VJSUhgwYABnzpwhKiqKRx55BLgxIouIiGDJkiUMHz6cBQsWAPDAAw+wZcsWMjIy8Pf3d2orJCSEHTt2sG7dOpYvX05oaKijnxaL5ZZ9zE9f70RGho1Ll67flbb/qs6du1LcXRCRu6RcOW88PO4sngo1UitVqhRwY+l9XuxltxrNzZ49m8OHDxMWFuYINAAPDw/Cw8OpW7cucXFxxMXFOcr8/PxyBJpd586dAdi7d69TP9PS0u64jyIiUvIVKtR8fHwoVaoUKSkpOVYtwo37z1JSUvD09KRs2bJ5trN9+3YAHn300Rxl7u7utG3bFvh9Gf/t2K+R2UPM/rSQ5OTkXOvf7pqbiIjcGwoVahaLhfr165OVlcXRo0dzlB85coTs7Gyn+9dyc/nyZQBcXV1zLbdvz8zMBGD16tWMGjWKf//737nWT0pKAqBq1aoAjv3bV0H+0aFDh5zqiYjIvanQ96nZn+24bt26HGX2bR07drxlG/Yl9Rs3bsxRlpWVxS+//AJAQEAAcGOV4nfffcfXX3+do75hGKxcuRLAsWKyXr16+Pv7s3//fk6dOuVUPzs7m/Xr12OxWAr0nEoRESl5Ch1q/fr1w9PTk5kzZzquYQHs2bOHWbNm4eXlxYABAxzbjx07xqFDh7hy5fcL/fZnQ0ZGRhIfH+/YbrPZ+Pjjjzl48CANGjSgdevWAPTq1QsfHx/i4+OZO3euo75hGEybNo1du3ZhtVodN1UDPPvss2RlZfGPf/zDaZXjZ599xtGjRwkMDKRWrVqFPR0iIlKMLEYRLPlbsGABERERuLu707p1awzDIDY2FpvNxkcffUSfPn0cdbt06cKJEyf48MMP6devn2P7v/71L7744gssFgvNmjXD19eXAwcOcPLkSSpVqkR0dDT169d31F+zZg2jRo0iMzOTBg0acN9995GQkMDRo0fx8/Nj/vz51KlTx1Hf/iiv+Ph4/Pz8aN68OUeOHOHgwYNUr16dRYsW3bUn9Wv1Y9EYMuT3L0dz5nxVjD0Rkbup2FY/2g0cOJDIyEgefPBB4uPj2bt3L82bNycqKsop0G5l1KhRREZG0rZtWw4dOsSmTZuwWCwEBwezfPlyp0AD6N69OwsXLiQoKIjk5GTWr19PZmYmgwYNYuXKlU6BBjdWUs6ePZuwsDC8vb2JiYkhNTWV/v3739VAExGRP0+RjNTk1jRSKxoaqYn8NRT7SE1ERKQkUKiJiIhpKNRERMQ0Cv2Xr0VEitMPP6xixYqlpKfn/hi8vxpPTy/69HmSHj16FXdXioVGaiJyT1uzZpUC7Sbp6WmsWbPq9hVNSqEmIve07t174enpVdzdKDE8Pb3o3v2vOUoDTT+KyD2uR49exT7VpttNSg6N1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaejm6xLu5ps65Xc6L7/Tzb4iv9NITURETEOhJiIipqHpx3tImfufLe4uSAlx5cDC4u6CSImkkZqIiJiGQk1ERExDoSYiIqahUBMREdNQqImIiGko1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaSjURETENBRqIiJiGgo1ERExDf2R0HuI/jCkiMitaaQmIiKmoVATERHT0PTjPaTM/c8WdxekhNBUtEjuNFITERHTUKiJiIhpKNRERMQ0FGoiImIaCjURETENhZqIiJiGQk1ERExDoSYiIqahUBMREdNQqImIiGkU2WOyfv75ZyIjI0lISCAzM5PGjRsTGhpK+/bt893Grl27mD59Ojt37uTatWtUrVqVLl26MHz4cMqVK5ejfmJiIjNmzCA2NpYLFy7g4+ND8+bNCQ0NpVmzZjnqv/DCC/zyyy957n/t2rXUrl073/0VEZGSpUhCbdmyZYwbNw4PDw9at25NdnY2sbGxDB06lIiICPr373/bNtatW8fIkSOx2Ww0bdoUPz8/9uzZQ3R0NBs2bGDhwoX4+vo66sfGxvK3v/2N9PR06tWrxwMPPEBSUhI//fQTGzdu5JNPPqFnz55O+/jtt98oVaoUXbt2zbUPpUuXLtyJEPkLGTJkQHF3oUTSefndnDlf/en7LHSonT17lvDwcMqUKcNXX32F1WoFYPfu3QwePJj333+fTp06UaVKlTzbsNlshIeHk52dzdSpUwkKCgIgPT2dkSNHEhMTw7Rp0xg/fjwAmZmZvPXWW6SnpzN+/HiCg4Mdba1cuZIxY8bw9ttv07p1a0cQnjhxgosXL9K2bVsmTpxY2MMWEZESqNDX1ObPn09GRgYhISGOQANo2rQpQ4cOJT09nUWLFt2yjYSEBJKTkwkICHAEGoCnpydhYWEA7Nixw7H9l19+4eTJk7Ro0cIp0AAef/xxunbtSmpqKhs3bnRsP3DgAACNGze+84MVEZESrdAjtc2bNwPQrVu3HGWBgYFMnjyZTZs28eqrr+bZhovLjWw9f/48NpsNN7ffu5WSkgLgdE0tLS2NJk2a5Hm9rk6dOsCNUaTd/v37AYWayN3wTvu8Z2Lkr+WdzWeKdf+FCjXDMEhMTMTFxYX77rsvR3mdOnVwcXEhMTERwzCwWCy5tlO/fn2qVavGqVOnGDNmDK+99hp+fn7s2rWLCRMm4OLiwuDBgx31AwMDCQwMzLNfe/bsAXCa8rSH2qVLl3jxxRfZt28f6enpNGnSpMALWkREpGQq1PTjpUuXyMjIoHz58nh4eOQod3Nzo0KFCly/fp3U1NQ823F3d2fKlClUqVKFVatWERgYSLNmzQgJCSEzM5OZM2fSpUuXfPVp27ZtxMbG4uXlRYcOHRzb7dOP4eHhnD17lpYtW1KjRg22b9/O0KFDiY6OLuDRi4hISVOokdr169cB8Pb2zrOOl5cXAKmpqfj4+ORZr1atWvTu3ZuoqCgaN25MxYoV2bt3L2fPnmX27Nk0adKE8uXL37I/SUlJjB49GoDQ0FDHIpELFy5w+vRp3Nzc+Oijj3jsscccr1m9ejWjR4/mo48+olWrVtx///35O/gC8PBww8+vTJG3KwLovSUlVnG8NwsVavZrYbdiGMZt66SkpDBgwADOnDlDVFQUjzzyCAAZGRlERESwZMkShg8fzoIFC/Js49ixY4SEhHDu3Dk6derEyy+/7Cjz9fVl27ZtXL582XG9za5nz57s2rWL6Ohovv76ayIiIm7bXxERKZkKFWqlSpUCbiy9z4u97FajudmzZ3P48GFGjx7tCDQADw8PwsPDiYuLc/zTokWLHK/fvXs3L7/8MsnJybRr144pU6bkCFxfX1+n+9xu1rlzZ6Kjo9m3b1/eB1sIGRk2Ll26flfaFjl37kpxd0EkV3f63ixXzhsPjzuLp0JdU/Px8aFUqVKkpKRgs9lylNtsNlJSUvD09KRs2bJ5trN9+3YAHn300Rxl7u7utG3bFvh9scfNfvrpJ55//nmSk5Pp2bMn06dPx9PTs0DH4efnB9xYVSkiIveuQoWaxWKhfv36ZGVlcfTo0RzlR44cITs72+n+tdxcvnwZAFdX11zL7dszMzOdti9dupRXXnmF69evM3jwYD799NNcF6z8/PPPjB49mrlz5+baflJSEgBVq1a9ZT9FRKRkK/TN1/al8OvWrctRZt/WsWPHW7Zhvx3g5pul7bKyshzPawwICHBq++233yY7O5u33nqLt956K89bBtLS0li5ciVffvllriPKb7/9FoB27drdsp8iIlKyFTrU+vXrh6enJzNnzmTv3r2O7Xv27GHWrFl4eXkxYMDvz0I7duwYhw4d4sqV3+da7c+GjIyMJD4+3rHdZrPx8ccfc/DgQRo0aEDr1q0BOHfuHOPGjSM7O5s33njD6R623LRr1w5/f39OnDjBJ598QlZWlqNs6dKlfP/99/j5+fHUU08V7mSIiEixKvQTRWrUqMHYsWOJiIjg2WefpXXr1hiGQWxsLDabjY8++oiKFSs66oeEhHDixAk+/PBD+vXrB9wYyYWGhvLFF18wcOBAmjVrhq+vLwcOHODkyZNUqlSJyZMnO6Yho6OjuXz5Mu7u7iQkJPDmm2/m2regoCCCgoLw8PBg4sSJvPjii8ydO5f169cTEBDA8ePHOXDgAKVKlWLq1KmUKaOl0SIi97IieUr/wIEDqV69OrNmzSI+Ph4PDw+aN2/Oyy+/TJs2bfLVxqhRo2jevDnz5s1jz5497N27l8qVKxMcHMywYcOoXLmyo659YUlmZibfffddnm3Wrl3b8SzJ5s2bs3z5cqZPn87WrVuJiYmhQoUK9OvXj7CwMGrWrFmIMyAiIiVBkf09tc6dO9O5c+fb1lu/fn2h21i8eHGB+mZXp04dPvroozt6rYiIlHz6y9ciImIaCjURETGNIpt+lLvvyoGFxd0FEZESTSM1ERExDYWaiIiYhqYfS7g5c74q7i6UGEOG/H4Tv86LiORGIzURETENhZqIiJiGQk1ERExDoSYiIqahUBMREdNQqImIiGko1ERExDQUaiIiYhq6+VpECu2dzWeKuwsigEZqIiJiIgo1ERExDU0/ikihvdO+SnF3QUqI4p6K1khNRERMQ6EmIiKmoVATERHTUKiJiIhpKNRERMQ0FGoiImIaCjURETENhZqIiJiGQk1ERExDoSYiIqahUBMREdNQqImIiGko1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIabgVdwdE5N73zuYzxd0FEUAjNRERMRGFmoiImIamH0XkjsyZ81Vxd6HEGDJkgOO/dV6Kl0ZqIiJiGgo1ERExDYWaiIiYhkJNRERMQ6EmIiKmoVATERHTUKiJiIhp6D41ua0ffljFihVLSU9PK+6uONx8X9CfzdPTiz59nqRHj17F1gcRyV2RhdrPP/9MZGQkCQkJZGZm0rhxY0JDQ2nfvn2+29i1axfTp09n586dXLt2japVq9KlSxeGDx9OuXLlctS/fPkyM2bMYN26dZw6dYpKlSoRFBTEiBEj8PHxyVE/LS2N6OhoVq5cSVJSEmXKlKFTp068+uqrVK5cuVDHb2Zr1qwqUYFW3NLT01izZpVCTaQEKpLpx2XLljF48GB27txJ06ZNeeihh9i5cydDhw5l0aJF+Wpj3bp1DBw4kA0bNlC7dm06dOhAeno60dHRPP3001y4cMGp/tWrVwkODmbWrFlYLBY6deqExWIhKiqK/v37c+XKFaf6mZmZhIWF8emnn5KamkrHjh0pX748S5YsoV+/fpw8ebIoToUpde/eC09Pr+LuRonh6elF9+4KNJGSyGIYhlGYBs6ePUvXrl3x9PTkq6++wmq1ArB7924GDx5MZmYmP/74I1WqVMmzDZvNRseOHblw4QKfffYZQUFBAKSnpzNy5EhiYmIIDg5m/Pjxjte89957zJs3j2eeeYYJEybg4uKCzWbj73//OytWrMhRPyoqin/+85906tSJqVOn4uHhAcCkSZOIjIykc+fOREZGFuZU5Ckjw8alS9fvStsiUvz0mKyiVa6cNx4edzaRWOiR2vz588nIyCAkJMQRaABNmzZl6NChpKen33a0lpCQQHJyMgEBAY5AA/D09CQsLAyAHTt2OLZfvnyZJUuW4OPjw9ixY3FxuXEYbm5uhIeHU65cOb755huuXbsGgGEYREVFYbFYGD9+vCPQAEaOHEndunWJiYnh+PHjhT0dIiJSjAodaps3bwagW7duOcoCAwMB2LRp06078f+hdP78eWw2m1NZSkoKgNM1tR07dpCWlkbr1q1zXDsrXbo0bdq0IS0tzRGECQkJnDlzhoCAAGrUqJFj3126dMlXP0VEpGQrVKgZhkFiYiIuLi7cd999Ocrr1KmDi4sLiYmJ3GqWs379+lSrVo0zZ84wZswYjh07xvXr19m2bZtjanHw4MGO+omJiQA0aNAg1/bsfUlISChQ/YMHD97ukEVEpAQr1OrHS5cukZGRga+vr9OUnqNxNzcqVKjA+fPnSU1NzXVFIoC7uztTpkxhxIgRrFq1ilWrVjnKKleuzMyZM2nXrp1j27lz5wDw8/PLtT379vPnz99RfRERuTcVKtSuX7+x+MHb2zvPOl5eN1bN3SrUAGrVqkXv3r2JioqicePGVKxYkb1793L27Flmz55NkyZNKF++PIDjWlle+7Xv016voPWLmoeHG35+Ze5K2yJSsuh3vXgVKtTs18JuJT+LK1NSUhgwYABnzpwhKiqKRx55BICMjAwiIiJYsmQJw4cPZ8GCBU77tVgst9yn/d8FrS8iIvemQoVaqVKlgBtL7/NiL7vVaG727NkcPnyY0aNHOwINwMPDg/DwcOLi4hz/tGjRwrHftLTcbwj+4z4LWr+oaUm/yF/HuXNXbl9JbqnYlvT7+PhQqlQpUlJScqxahBv3n6WkpODp6UnZsmXzbGf79u0APProoznK3N3dadu2LQD79+8HcDz9Izk5Odf2/ngNraD1RUTk3lSoULNYLNSvX5+srCyOHj2ao/zIkSNkZ2c73b+Wm8uXLwPg6uqaa7l9e2ZmJvD7Kkb7qsY/OnToEAANGzYEcOz/dvVv108RESnZCn2fmv3ZjuvWrctRZt/WsWPHW7ZhX1K/cePGHGVZWVn88ssvAAQEBADQsmVLvLy82LZtW47FHampqWzbto1SpUrx8MMPA1CvXj38/f3Zv38/p06dcqqfnZ3N+vXrsVgsBXpOpYiIlDyFDrV+/frh6enJzJkz2bt3r2P7nj17mDVrFl5eXgwY8PsjZI4dO8ahQ4ecns3Yv39/ACIjI4mPj3dst9lsfPzxxxw8eJAGDRrQunVr4MY1sr59+3Lp0iUmTJjgmPq02WxERERw+fJl+vfv77Ta8tlnnyUrK4t//OMfTkH42WefcfToUQIDA6lVq1ZhT4eIiBSjQj/7EWDBggVERETg7u5O69atMQyD2NhYbDYbH330EX369HHU7dKlCydOnODDDz+kX79+ju3/+te/+OKLL7BYLDRr1gxfX18OHDjAyZMnqVSpEtHR0dSvX99R/+LFizz77LMcOXKEmjVr0qhRI/bv38/x48dp1KgR8+fPp3Tp0o769kd5xcfH4+fnR/PmzTly5AgHDx6kevXqLFq06K49qV8LRUTMTc9+LFrF+uxHgIEDBxIZGcmDDz5IfHw8e/fupXnz5kRFRTkF2q2MGjWKyMhI2rZty6FDh9i0aRMWi4Xg4GCWL1/uFGgA5cuXZ+HChQwaNAibzUZMTAwuLi4MHTqUL7/80inQ4MZKytmzZxMWFoa3tzcxMTGkpqbSv3//uxpoIiLy5ymSkZrcmkZqIuamkVrRKvaRmoiISEmgUBMREdNQqImIiGko1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaSjURETENBRqIiJiGgo1ERExDf3pmT+B/vSMyN3zww+rWLFiKenpacXdlRLB09OLPn2epEePXsXdlTumPz0jIn9Za9asUqDdJD09jTVrVhV3N4qNQk1E7mndu/fC09OruLtRYnh6etG9+707SissTT/+CTT9KCKSf5p+FBERQaEmIiImolATERHTUKiJiIhpKNRERMQ0FGoiImIaWtL/J8jONrDZsoq7GyIi9wQ3N1dcXCx39FqFmoiImIamH0VExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaSjURETENBRqIiJiGgo1ERExDYWaiIiYhkJNRERMQ6EmIiKmoVATERHTUKiJiIhpKNRERMQ0FGoi4sQwjOLuwp/qr3a8ZqdQExGHVatW8eabbzr+f9myZTRs2JB//OMf+W6jYcOGNGrUqEj68z//8z80bNiQqVOn3nEbSUlJNGzYkMDAQKftR48e5cUXX+TEiROF7aaUIAo1EQHg119/5Y033uDs2bPF3ZU/xbBhw9iyZUtxd0OKmFtxd0BESobs7OwiaWf16tVYLJYiaasoVKlShdWrV+Ph4eG0vaiOV0oWhZqIFKl69eoVdxecuLu7l7g+yd2jUJM7snHjRqKjozl48CCXLl2iSpUqtGvXjmHDhlGtWjWnut9//z1ff/01CQkJZGVlUbduXQYOHMjjjz+Oi4vzDPiKFStYtGgRv/32Gzabjdq1a/PYY4/xwgsv4OXl5ai3bNkyxo0bx9tvv82pU6dYtGgRAL179+add94B4NKlS3zxxRf8+OOPnDp1itKlS9OqVSvCwsIICAhw2m92djZffvkl3333HUePHiUrK4tatWrRo0cPQkJC8Pb2vgtnseR46623WL58OQDbt2+nYcOGPPHEE7Rq1cpR59dff2XatGns3LkTgPvvv59hw4bRoUMHp7YaNmyIq6sr+/fvB25c0+ratSvdu3cnMDCQTz75hIsXL9KgQQMWLlyIu7s7ly5dIjIykjVr1pCcnEzdunUZNmxYkRybff+1atXixx9/JDY2lueff95R3rVrVwASEhIc206fPs306dPZtGkT586do3z58rRr147hw4dTs2bNHMfbuHFjRo8eTXh4OKdOnaJGjRrMmzePSpUqFbi/hw4d4vPPP2f37t2cOXOGcuXK0bx5c4YMGcJDDz2Uo/7u3buJiooiLi6OK1eu4O/vT48ePRg8eDA+Pj5OdX/99Vdmz55NfHw8V69epXLlynTo0IGXXnqJqlWr5jhnt/qZ2Ww2Fi1axNKlSzl8+DAuLi40atSI559/nqCgoBz9LMhnRmHompoU2I8//shLL73Ejh07aNCgAZ07dwbg66+/5qmnnuLcuXOOuv/93//Na6+9xs6dO2nSpAktWrQgMTGRsWPH8sEHHzjqZWdnM7UcLUIAAA+BSURBVGrUKMaMGcPevXt56KGHaN++PWfPnuXTTz9lwIABXL58OUdf5s+fT3R0NC1atKBOnTrUrVsXgJMnT/Lkk08ya9YsbDYbHTp0oE6dOqxdu5ann36amJgYp3b++c9/8uGHH5KUlESLFi1o3bo1p0+fZvLkyYSGhpp+hdxDDz1Eu3btAKhYsSK9e/d2+gDdsWMHgwYN4siRI7Rt2xZ/f3/i4uIIDQ1l48aN+drH/v37GTt2LNWrV6dly5b4+/vj7u5OSkoKAwcOZM6cORiGQadOnTAMg9dff51Vq1YV+bFWqlSJ3r17U6pUKQC6detG7969nfrZt29fFi5ciKenJ507d8bPz4/ly5fTr18/du/enaPNs2fPEhYWhre3N48++ihly5a9o0A7duwYwcHBrF69mooVK9KlSxeqVavG2rVrCQ4OZuvWrU71v/32WwYMGMDq1aupXr067du35+rVq3z++ecMGTKE9PR0R90FCxYwcOBA1q1bR+3atenSpQtubm58/fXX9O3bl3379uXoT14/s8zMTF566SUiIiIcvzMPPfQQu3fv5pVXXmHSpElO7RTkM6PQDJEC6tq1q9GoUSMjMTHRsc1msxmvv/66YbVajSlTphiGYRhr1qwxrFar0aVLF+PYsWOOuqdOnTI6dOhgWK1WY9euXYZhGEZ0dLRhtVqNoKAg4/jx4466V65cMUJDQw2r1Wq89tprju1Lly41rFarYbVajXXr1jm2Z2VlGYZhGM8995xhtVqNTz75xLDZbI7yLVu2GA888IDx8MMPG8nJyYZhGMaJEycMq9VqdO/e3bh69aqj7sWLF42goCDDarUav/zyS5Gcu5Jsx44dhtVqNYKDgx3bbj7P7733nuNcZmdnG++8845htVqNkJAQp3asVqtx//33O/7/+PHjjjY++OADx3b7z2rChAmG1Wo1Ro4caaSnpzvKZ8yY4Xid/T11J+z779atm9P2bt26GVar1en9lp6ebnTt2tWwWq3GvHnznOovX77caNiwodG5c2enftr7OGLECCM7O9vp2Apq3LhxhtVqNRYvXuy0ff78+Tl+NidPnjSaNWtmNG7c2Ni4caNje1pamvHiiy8aVqvVmDFjhmEYhrFv3z4jICDAePDBB42tW7c66mZlZRlTp041rFar03Hd7mc2adIkw2q1GoMHDzZSUlIc5cePH3ec182bNzu25/czoyhopCYFdu7cOdzc3PDz83Nsc3V15fXXXyc8PNzpWxjA22+/7TRlU7VqVYYPH06DBg04cuQIAHPnzgVujJhq1KjhqOvj48PEiRMpU6YM33//PSdPnnTqi7+/v2P6CMDFxYVdu3YRHx9P48aNGTVqFK6uro7yRx99lIEDB3LlyhW++eYbAJKTkwEoX748pUuXdtQtV64c7777Lh988EGOKae/mipVqjB27FjHubRYLAwZMgRwnra7nZun/VxcXMjIyGD58uV4eXkRERHhtJgjNDSUBx98sIiOIH9+/PFHjh8/TmBgIMHBwU5lffv2JSgoiBMnTrB27docrx00aJBjgcwfp9Xzyz5iuXkqEKB///78/e9/Z+jQoY5t3377LdeuXWPQoEFOU8Cenp689dZb1KxZk/PnzwMwb948srOzefnll2nbtq2jrouLCyNGjKBVq1acOHGC1atX5+hTbj+z+fPn4+npyccff0z58uUd5TVq1HDc/hEVFeV0XPn5zCgKCjUpsBYtWpCWlsZTTz3FtGnT2Lt3L4ZhULNmTQYMGECTJk0wDIMdO3bg7u7umNa62TPPPMN3331H3759OXXqFCdOnKBq1aq5XjMoU6YMHTp0cLR5sz9eGwOIjY0FoFWrVrmuwmvfvj1w49oRQIMGDShfvjw7d+5k4MCBLFiwgOPHjzvaePLJJ6levXoBz5K5NG3aFDc350vw9usguU0L56ZMmTL4+/s7bduzZw/Xrl2jWbNmlC1bNsdrbv7C8mewv3ceeeSRXMv/+N65WW7vxYJq2bIlAK+//jrvvfceW7duJSMjAzc3N1544QU6duzoqGvvQ26BUL9+fdatW8e4ceMAHL83//Vf/5Xrfnv16uVUzy63n9m+ffu4cuUK9evXz3WKtU2bNri5uREfH09WVhaQv8+MoqKFIlJg7777LmFhYRw4cIApU6YwZcoUKlasSOfOnenfvz9NmzYlJSWFzMxMqlWrhru7+y3bs98X9cdfnpvZR2/2UZVduXLlctQ9deoUcOOb4s3fFv/o9OnTAHh7ezN58mTeeOMN4uLiiIuLA6Bu3boEBQUxYMCAHN+c/2rKlCmTY5s95PK7ND63n5X9Z1+lSpVcX3Or98TdYH/vvPfee7z33nt51rO/d+xcXFxyDeWCGjx4MAcOHGD16tXMmzePefPm4e3tTZs2bXjiiSecFmDYR3X5WWRxu9+xO/n92rdvHw0bNsxznzabjUuXLuHr65uvz4yiolCTAqtevTrLli0jNjaWn376iW3btpGYmMg333zD0qVLGT9+fK6rn/Ji/P8ijFvd22T/xvfHe41ym+axf8g2a9bsltOGvr6+jv9u06YN69evJyYmhg0bNrBt2zaOHDnCjBkzmDdvHtHR0UX6i3evudPptMK2cfPU8Z/B/t5p27YtFStWzLNe/fr1nf6/qO7Lc3d3Z9KkSbz88susXbuWrVu3smfPHtavX8/69evp0aMHn332GXAjNPLLuM1Cpzv5/apRo0auMyu5yc9nxsCBA/PV1u0o1OSOuLi40KZNG9q0aQPAmTNnmDdvHjNnzmTixIk8/fTTuLu7k5ycjM1myzF1lZKSwpo1a2jYsKHjW7p9yi839rJbfdDY2eftO3bsSFhYWL6Pydvbm549e9KzZ08AfvvtNyZNmsSGDRv47LPPmD17dr7bkvyxj4D/eK3UrkhXxeWD/b3Tt29f+vTp86fu+2ZWqxWr1cqIESO4evUqa9euJSIigh9++IFdu3bRrFkz/Pz8OHLkCKdPn871y9uSJUvw9fWlS5cuVK5cmaSkJJKSkqhdu3aOuklJSUDBfr9q1qzJxIkT831Mt/vMeOaZZ247q5Ov/RS6BflLOXr0KL179yY0NNRpe5UqVXjzzTepUKEC165d48qVKzRp0oTMzEy2bduWo50NGzYQHh7OypUrqV69Ov7+/pw5c8ZxD9TNrly5wtatW3FxcaFFixa37aO9zqZNm3L9hrpw4UIee+wxpk2bBty4jy4wMJDIyEinegEBAY7nINqnXMysOJ4C0qRJE8qWLcuuXbtyTH0B+b5d4E7kdrw3v3dyM3nyZPr06cPixYuLvD+GYRASEkL79u2dluL7+PjQr18/x2IQ+xeA5s2b59nXEydO8Pbbb/Ppp59isVgc1+p++OGHXPf9/fffAzjdl5iXBx54AC8vL/bs2cOFCxdylCckJBAYGMgrr7yCYRj5/szI77XZ21GoSYHUrFmTCxcusHnzZtatW+dUtnXrVlJSUvD396dixYqO6YT333+fM2fOOOqdPn2aKVOmYLFYHPcHvfDCC8CNm4Dt3xoBUlNTGT16NFevXiUoKIjKlSvfto+tW7cmICCAnTt3MmnSJKdpmv379zNp0iT+85//OK4H1KtXj2PHjvHll1/yv//7v05tfffdd8CNX2Sz8/T0BG58ifizuLu7M2DAADIzMxk7dizXrl1zlC1evPiuPpvRfrxXr151bOvVqxd+fn589913LFiwwKn+5s2bmT17NgkJCXfl/WCxWChbtixnz55l8uTJTtcqT58+TXx8PC4uLo5FFU899RQeHh7MmzfPaYFHWloaERERADz++OMABAcH4+rqyvTp052+ZBqGweeff86OHTvw9/fP1yrEUqVK8fTTT3P16lXGjBlDSkqKoywlJYVx48Zx7NgxqlWrhsViKdBnRlHQ9KMUiKurKxMmTGDEiBEMHz6cJk2a4O/vz7lz59i5cyeurq6MHz8euPF0j59//plly5bRo0cPWrVqRVZWFvHx8Vy7do2XXnrJ8W1z0KBB7Ny5k++//56ePXvSsmVLvL29iYuLIyUlhUaNGjmeFHI7FouFTz/9lBdeeIEZM2awYsUKGjVqxNWrV4mLiyM7O5tBgwbRrVs34MZUT0hICHPnzqVXr148/PDDlCtXjsTERA4dOkSlSpV45ZVX7sr5LEn8/f1xc3PjwIEDDBkyhJYtW+a5gKMovfzyy8TFxbFlyxYCAwN5+OGHOXnyJHv27KFZs2bs2rXrruy3du3aHDx4kFdffZWAgAA++OADfHx8mDx5MsOGDSMiIoLo6GgaNGhAcnKyox9vvfUW999//13p0+jRo4mNjWXOnDn8+OOPBAQEcP36deLj47l+/Tp/+9vfqFWrFnDjC2Z4eDjjx4/n+eefd7xvd+/ezdmzZ2nVqhUvvvgicGNEPG7cON5//31CQkJ46KGHqFKlCr/99htHjx7F19eXyZMn5/vJOaNGjWLfvn1s3ryZwMBAx+rYuLg4UlNTadasGa+99hpQsM+MoqBQkwLr1q0bs2bNYu7cuezZs4fffvuN8uXLExQURGhoqNPy3A8++IBWrVqxaNEitm/fTlZWFlarleDgYPr27euo5+LiwqRJk+jQoQOLFy/m119/BaBOnTqEhoYSHByc4yL2rdSrV49vv/2WmTNnEhMTw5YtWyhTpgwtWrQgODg4x0KWsWPHUrt2bZYvX87u3bvJzMykSpUqBAcH89JLLzndX2NWFSpU4N133+Xzzz9n+/btZGZm8sQTT9z1/Xp5eTF79mzmzJnDihUriImJoXr16rzzzjuUKlXqroXamDFjuHDhAnv37uXixYskJSUREBBAixYt+Pbbb5kxYwZbtmxh48aNVKhQgQ4dOjBkyBDHNaG7oWbNmixcuJDp06cTGxtLTEwM3t7ePPDAAzz33HOO6712Tz31FHXq1GHWrFns3LmT1NRUqlevzvDhwwkNDXW6lj1o0CDuv/9+R919+/ZRrVo1QkJCGDJkSIG+wHh7exMdHc1XX33FypUr+fXXX3F1daV27dr07t2b5557zikgC/KZUVgW43bLYkRERO4RuqYmIiKmoelHEbknHDp0iOnTpxfoNS1btqR///53qUd5i4uLY+HChQV6TVBQUIHu75TcKdRE5J6QnJzMv//97wK9xs3NrVhC7dixYwXua+3atRVqRUDX1ERExDR0TU1ERExDoSYiIqahUBMREdNQqImIiGko1ERExDQUaiIiYhoKNRERMQ2FmoiImIZCTURETEOhJiIipqFQExER01CoiYiIaSjURETENP4PryvUMLt+0z4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in scores from 3rd iteration\n",
    "third_iter_scores = np.load('./data/iter03.data.npy')\n",
    "\n",
    "df = pd.DataFrame(data=list(zip(scores, third_iter_scores)),\n",
    "                  columns=['scores','third_iter_scores'])\n",
    "plt.figure(figsize=(3,2), dpi=144)\n",
    "sns.boxplot(data=df)\n",
    "plt.savefig(fname='4_vs_3.png', dpi=120);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median values of one box just barely overlap the color area (IQR) of the other.  Arguably the new model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Cross Validated Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "# previous model\n",
    "print(f'Previous Cross Validated Accuracy: {third_iter_scores.mean() :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Summary\n",
    "\n",
    "In this iteration we:\n",
    "* looked at the string variables not yet incorporated into the model\n",
    "* determined that title and Embarked looked promising and encoded them as dummy variables\n",
    "* compared new model with previous iteration\n",
    "* simplified the model by removing several extracted features that were not helping\n",
    "* changed the encoding for passenger class\n",
    "\n",
    "The mean CV score improved from 81.8% in the previous iteration to 83.2% in this iteration.\n",
    "\n",
    "At this point, it could be argued that the initial model is complete and continually refining it further, given that there is no hold-out test set, requires care to ensure that additional iterations do not overfit the data.  \n",
    "\n",
    "The next notebook discuss hyperparameter optimization and Nested Cross Validation as a means to ensure the further optimized model does not have a score that is optimistically biased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
