{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2 <br/>*Imputation and Cross Validation*\n",
    "\n",
    "Jupyter Notebook referenced from my website:\n",
    "[Software Nirvana: Cross Validation (2)](https://sdiehl28.netlify.com/2018/03/cross-validation-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "1. Iteratively improve upon model using Age Imputation\n",
    "2. Demonstrate the right and wrong way to perform Cross Validation\n",
    "3. Compare this model with the previous iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where We Are\n",
    "In the first iteration, we created a simple model and showed that the accuracy was better than the null model.  The null model is the model that predicts the predominant class in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "<a href=\"https://en.wikipedia.org/wiki/Imputation_(statistics)\">Imputation on Wikipedia</a>\n",
    "\n",
    "This notebook will impute the missing values for Age and use Age as an additional attribute for prediction.  We will also check to see if adding the Age variable improved prediction accuracy.\n",
    "\n",
    "Special attention will be paid to avoid a common beginner's mistake, which is to look at the test data when performing imputation or other preprocessing steps.  The easiest way to ensure there is no \"test set leakage\", is to use a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Model Building Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      int64\n",
       "Age       float64\n",
       "SibSp       int64\n",
       "Parch       int64\n",
       "Fare      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# break up the dataframe into X and y\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "\n",
    "# As before, remove all non-numeric fields and PassengerId\n",
    "drop_fields = ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'PassengerId']\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age: Cross Validation the Right Way\n",
    "This will be performed manually to emphasize how preprocessing operations work on the folds in a cross validation.  The key is that the held-out data, aka the test data, can never be looked at.\n",
    "\n",
    "Later this will also be performed with an Imputer and a Pipeline to show a more concise workflow.\n",
    "\n",
    "To a beginner, it can appear that using an Imputer and a Pipeline is a lot of extra work.  Why not just impute the Age before cross validation and be done with it.  If you were to look that at the [\"Kernels\"](https://www.kaggle.com/c/titanic/kernels?sortBy=votes&group=everyone&pageSize=20&competitionId=3136) on Kaggle posted for the Titantic dataset, you would see that most people do just that.  But this could lead to an estimate of model accuracy that is too high.  Looking at the test data prior to training your model is called \"data leakage\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age Manually (no Pipeline)\n",
    "A nice introduction to overfitting, train/test split, and cross validation in Python is:[Train/Test Split and Cross Validation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "\n",
    "K-Fold Cross Validation *is* train/test split, but performed K times to get a more accurate estimate of model accuracy.\n",
    "\n",
    "We train the model on the train data and we test the accuracy of the model on the test data.\n",
    "\n",
    "With a train/test split, we have missing Age values in both the train and test sets.  \n",
    "\n",
    "For the train set, we replace the missing values with the average Age of the train set.  \n",
    "\n",
    "For the test set, we are not permitted to look at its data as the purpose of the test set is for model evaluation.  If we look at its data, we have made the mistake of \"data leakage\".  The test data has \"leaked\" into our model building process.  This may cause our estimate of model accuracy to be too high.  For the test set, we replace the missing values with the average Age of the *train* set. \n",
    "\n",
    "All of the above holds true for Cross Validation as well, as Cross Validation is just multiple train/test splits.\n",
    "\n",
    "For each of the K folds, we will compute the mean Age value in the train set, and use that value to replace the missing values in both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random_state to get the same folds each time we call KFold()\n",
    "random_state = 121212\n",
    "\n",
    "# for illustration only, 5 or 10 is the recommended value\n",
    "k_folds = 2\n",
    "\n",
    "# prepare for cross validation\n",
    "crossvalidation = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the K folds to impute Age and compute the accuracy score\n",
    "my_scores = np.zeros(k_folds)\n",
    "i = 0\n",
    "lr_model = LogisticRegression()\n",
    "for train_idx, test_idx in crossvalidation.split(X):\n",
    "    # train subset\n",
    "    X_train = X.iloc[train_idx, :].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    \n",
    "    # test subset\n",
    "    X_test = X.iloc[test_idx, :].copy()\n",
    "    y_test = y[test_idx].copy()\n",
    "    \n",
    "    # find the average age on the train set\n",
    "    train_age_mean = X_train['Age'].mean()\n",
    "    \n",
    "    # use this value for *both* the train and test set\n",
    "    X_train.loc[X_train['Age'].isnull(), 'Age'] = train_age_mean\n",
    "    X_test.loc[X_test['Age'].isnull(), 'Age'] = train_age_mean # Key Concept!\n",
    "    \n",
    "    # fit model on train\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict using model on test\n",
    "    predictions = lr_model.predict(X_test)\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    my_scores[i] = accuracy_score(y_test, predictions)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Pandas copy\n",
    "In the above, we had to use .copy() for our train and test sets.  This is critical to avoiding the Pandas warning: SettingWithCopyWarning.\n",
    "\n",
    "The imputation requires us to modify both the train and test sets for X.  However X.iloc[] and X.loc[] both return a *view* into X, not a copy of a subset of X.  If we try to use this view to modify data, we will get: SettingwithCopyWarning.\n",
    "\n",
    "In most cases this warning means your code will not do what you intended.  Therefore you should always write code that does not produce this Pandas warning.\n",
    "\n",
    "It can be difficult to discover why this warning was issued.  One way to track this down is to print out the .is_copy member of your dataframe.  If you find you get this warning when trying to modify data through a view (implemented as a weak reference) then you need to get an independent copy of your data using .copy() and the warning will go away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weakref at 0x7fd5a3940b38; to 'DataFrame' at 0x7fd5a6528dd8>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[train_idx, :].is_copy) # view into dataframe\n",
    "print(X.iloc[train_idx, :].copy().is_copy) # independent copy of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age with Pipeline\n",
    "Scikit Learn correctly uses the mean of the train set as the replacement value for missing values in both the train and test sets.  However this is all done behind the scenes.  The following is exactly the same as the above, but requires much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: same as above\n",
    "\n",
    "# Set random_state to get the same folds each time we call KFold()\n",
    "random_state = 121212\n",
    "\n",
    "# for illustration only, 5 or 10 is the recommended value\n",
    "k_folds = 2\n",
    "\n",
    "# prepare for cross validation\n",
    "crossvalidation = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  True\n",
      "Scores : [0.70852018 0.69438202]\n",
      "Cross Validated Accuracy: 0.701\n"
     ]
    }
   ],
   "source": [
    "# Use an Imputer and a Pipeline\n",
    "# Note: Age is the only column in X with null values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='mean')\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "classifier = make_pipeline(imputer, LogisticRegression())\n",
    "\n",
    "# cross_val_score() will properly compute \n",
    "# imputation and score per fold\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, \n",
    "                         scoring='accuracy', n_jobs=1)\n",
    "\n",
    "# Check to see that we got the same scores \n",
    "# as in the above for-loop\n",
    "print(\"Scores Match: \", (scores == my_scores).all())\n",
    "print('Scores :', scores)\n",
    "print(f'Cross Validated Accuracy: {scores.mean() :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both Methods Produce the Same Result\n",
    "The for-loop over each K fold train/test split is the same as using an Imputer and LogisticRegression in a Pipeline in cross_val_score()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age: Cross Validation The Wrong Way\n",
    "In my review of Kaggle Kernels published for the Titantic dataset most of the [\"Kernels\"](https://www.kaggle.com/c/titanic/kernels?sortBy=votes&group=everyone&pageSize=20&competitionId=3136) imputed the Age value over the entire data set, that is, they had \"data leakage\".\n",
    "\n",
    "For imputation, \"data leakage\" usually doesn't impact the estimate of model accuracy enough to make a practical difference.  Nevertheless, it is bad practice and if you decide to impute missing values across your entire dataset, you should explicitly note it as well as why you expect it will make little difference.\n",
    "\n",
    "In the following I will show that the approach to imputing missing values using the entire data set produces different results than the approach which avoids \"data leakage\" entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  False\n",
      "Scores Diff:  -2.519272434109432e-06\n",
      "Wrong Scores : [0.70627803 0.69662921]\n",
      "Cross Validated Accuracy: 0.701  SD: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Prior to Cross Validation:\n",
    "#   impute the missing values as the mean of *all* the data\n",
    "# This is \"data leakage\"!\n",
    "# Replace all null Age values with the mean of all Age Values\n",
    "X.loc[X['Age'].isnull(), 'Age'] = X['Age'].mean()\n",
    "\n",
    "# Setup: same as above\n",
    "random_state = 121212\n",
    "# for illustration only, 5 or 10 is the recommended value\n",
    "n_splits = 2\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, \n",
    "                        random_state=random_state)\n",
    "\n",
    "# Use cross_val_score()\n",
    "wrong_scores = cross_val_score(LogisticRegression(), X, y, \n",
    "                               cv=crossvalidation,\n",
    "                               scoring='accuracy', n_jobs=1)\n",
    "\n",
    "# We do *not* get the same scores as above\n",
    "# due to data leakage\n",
    "print(\"Scores Match: \", (scores == wrong_scores).all())\n",
    "print(\"Scores Diff: \", scores.mean() - wrong_scores.mean())\n",
    "\n",
    "print('Wrong Scores :', wrong_scores)\n",
    "print(f'Cross Validated Accuracy: {wrong_scores.mean() :.3f}  SD: {wrong_scores.std() :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing all the missing values, over both the train and test sets, prior to performing cross validation produced different results!  That said, the difference was much less than one standard deviation, so it was not significant.\n",
    "\n",
    "The use of the Imputer and estimator in the Pipeline, or the hand coded for-loop over the K folds, produced the correct (no data leak) result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation The Wrong Way: Discussion\n",
    "**What was wrong:** Used all data for imputation (did not have hold-out set).\n",
    "\n",
    "**What may happen:** Estimate of model accuracy may be too high.\n",
    "\n",
    "**Importance in Practice:  With *imputation*, this is usually not a problem** unless you have a very small amount of data on which to perform the imputation.  In the above, we saw it made almost no difference.\n",
    "\n",
    "**Importance in Practice:  With *feature selection*, this is usually a very serious problem** that leads to highly inflated values of model accuracy.  Feature selection is the process of determining which variables to include in the model.  If you use the entire dataset, and decide to keep variables based on some statistic of the data, such as correlation to the target variable, then you *must* use a Pipeline or otherwise ensure that you choice of variables is determined on a train set and evaluated on a test set.\n",
    "\n",
    "**Great Explanation and Story by Robert Tibshirani:**\n",
    "Robert Tibshirani, in the youtube video [Cross Validation: Right and Wrong](https://www.youtube.com/watch?v=S06JpVoNaA0&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf), explains the right and wrong way to perform cross validation in detail.  He also presents a wonderful story about a Ph.D. oral dissertation presenter filtering away variables *prior* to performing cross validation and the serious effect it had on his medical research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Score as per Previous Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.644 0.719 0.719 0.764 0.652 0.663 0.663 0.764 0.73  0.674]\n",
      "Cross Validated Accuracy: 0.699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Same as for previous iterations to allow for comparison\n",
    "k_folds = 10\n",
    "random_seed=5\n",
    "crossvalidation = KFold(n_splits=k_folds, shuffle=True, \n",
    "                        random_state=random_seed)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, \n",
    "                         scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# save scores for comparison with another iteration\n",
    "np.save(\"../data/iter02.data\", scores)\n",
    "\n",
    "print('Scores: ', np.round(scores, 3))\n",
    "print(f'Cross Validated Accuracy: {scores.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance with Previous Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.045 -0.044  0.011  0.022  0.022  0.022  0.022  0.022  0.034  0.067]\n"
     ]
    }
   ],
   "source": [
    "# Read in scores from 1st iteration\n",
    "first_iter_scores = np.load('../data/iter01.data.npy')\n",
    "\n",
    "# the scores in the ordered list correspond with each other as they were\n",
    "# perform on the same train/test split\n",
    "diff_scores = scores - first_iter_scores\n",
    "print(np.round(sorted(diff_scores),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fd5a39a0240>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at boxplot\n",
    "sns.boxplot(y=diff_scores)\n",
    "plt.axhline(0, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model wins:  8\n",
      "New Model ties:  0\n",
      "New Model loses: 2\n"
     ]
    }
   ],
   "source": [
    "# How many times is the new model better?\n",
    "print(f'New Model wins:  {(diff_scores > 0).sum()}')\n",
    "print(f'New Model ties:  {(diff_scores == 0).sum()}')\n",
    "print(f'New Model loses: {(diff_scores <= 0).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above boxplot and individual comparison of the scores, it appears the new model is slightly better.  The change we made was to impute the missing Age values by their mean value and make use of the Age value in the Logistic Regression Model.  It is reasonable that the model would improve with the addition of the Age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "Model building steps only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.644 0.719 0.719 0.764 0.652 0.663 0.663 0.764 0.73  0.674]\n",
      "Cross Validated Accuracy: 0.699\n"
     ]
    }
   ],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# break up the dataframe into X and y\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "\n",
    "# As before, remove all non-numeric fields and PassengerId\n",
    "drop_fields = ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'PassengerId']\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "\n",
    "k_folds = 10\n",
    "random_seed=5\n",
    "crossvalidation = KFold(n_splits=k_folds, shuffle=True, \n",
    "                        random_state=random_seed)\n",
    "\n",
    "imputer = Imputer(strategy='mean')\n",
    "classifier = make_pipeline(imputer, LogisticRegression())\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, \n",
    "                         scoring='accuracy', n_jobs=1)\n",
    "\n",
    "# Use the mean score as the best estimate of model accuracy\n",
    "print('Scores: ', np.round(scores,3))\n",
    "print(f'Cross Validated Accuracy: {scores.mean() :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Cross Validated Accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "# previous model\n",
    "print(f'Previous Cross Validated Accuracy: {first_iter_scores.mean() :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this iteration we:\n",
    "* showed the right and wrong way to setup cross validation\n",
    "* used Imputation as part of a Pipeline.  This improves the quality of the software as it concisely performs cross validation correctly\n",
    "* added the Age variable to our model\n",
    "* replaced missing Age values with the mean of the Age value\n",
    "* slightly improved the model's accuracy from 68.6% to 69.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
