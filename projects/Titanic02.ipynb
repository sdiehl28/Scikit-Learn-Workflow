{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2: Impute Age Without Test Data Leakage\n",
    "\n",
    "Jupyter Notebook referenced from my website: <a href=\"https://sdiehl28.netlify.com/projects/titanic/titanic02/\" target=\"_blank\">Software Nirvana: Titantic02</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where We Are\n",
    "In the first iteration, we created a simple model and showed that the accuracy was better than the null model.  The null model is the model that predicts the predominant class in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "This notebook will impute the missing values for Age and use it as an additional attribute for prediction.  We will check to see if adding the age variable improved prediction accuracy.\n",
    "\n",
    "Special attention will be paid to avoid a common beginner's mistake, which is to look at the test data when performing imputation or other preprocessing steps.  The easiest way to ensure that is no \"test set leakage\", is to use a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"outline\"></a>\n",
    "### Outline\n",
    "1. [Previous Iteration](#previous)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Preprocessing](#preprocess)\n",
    "4. [Model Building](#model)\n",
    "5. [Model Evaluation](#eval)\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"previous\"></a>\n",
    "### Previous Iteration\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (891, 11)\n",
      "y Shape:  (891,)\n"
     ]
    }
   ],
   "source": [
    "# break up the dataframe into X and y\n",
    "# X is a 2 dimensional \"spreadsheet\" of values used for prediction\n",
    "# y is a 1 dimensional vector of target (aka response) values\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "print('X Shape: ', X.shape)\n",
    "print('y Shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      int64\n",
       "Age       float64\n",
       "SibSp       int64\n",
       "Parch       int64\n",
       "Fare      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As before, removing non-numeric and PassengerId\n",
    "drop_cols = ['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
    "X = X.drop(drop_cols, axis=1)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age Value: With and Without Pipeline\n",
    "This will be performed manually, to emphasize that we are not looking at the test data, and with a scikit learn Imputer and Pipeline.\n",
    "\n",
    "The reason for using an Imputer and a Pipeline is to make it *eaiser* to write code which avoids looking at the test data, especially when using cross validation.\n",
    "\n",
    "To a beginner, it sometimes appears that using an imputer and a pipeline is extra work.  In order to see that it is not, it is helpful to correctly impute the age value with, and without using a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_save = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "29.69911764705882\n"
     ]
    }
   ],
   "source": [
    "random_state = 121212\n",
    "n_splits = 2 # use low number of splits to illustrate problem, this is not the recommended value\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "X = X_save.copy()\n",
    "print(X['Age'].isnull().sum())\n",
    "print(X['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.970521739130433\n",
      "29.445365853658537\n"
     ]
    }
   ],
   "source": [
    "# the following is very similar to the example at the end of the previous notebook\n",
    "my_scores = np.zeros(n_splits)\n",
    "i = 0\n",
    "lr_model = LogisticRegression()\n",
    "for train_idx, test_idx in crossvalidation.split(X):\n",
    "    # train subset\n",
    "    X_train = X.iloc[train_idx, :].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    \n",
    "    # test subset\n",
    "    X_test = X.iloc[test_idx, :].copy()\n",
    "    y_test = y[test_idx].copy()\n",
    "    \n",
    "    # find the average age on the test set\n",
    "    train_age_mean = X_train['Age'].mean()\n",
    "    print(train_age_mean)\n",
    "    \n",
    "    # use this value for *both* the train and test set\n",
    "    X_train.loc[X_train['Age'].isnull(), 'Age'] = train_age_mean\n",
    "    X_test.loc[X_test['Age'].isnull(), 'Age'] = train_age_mean # THIS IS THE KEY STEP TO UNDERSTAND\n",
    "    \n",
    "    # fit model on train\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict using model on test\n",
    "    predictions = lr_model.predict(X_test)\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    my_scores[i] = accuracy_score(y_test, predictions)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same with an Imputer and a Pipeline\n",
    "# Note that the only column with null values, in X, is the numeric value Age\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "classifier = make_pipeline(imputer, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, scoring='accuracy',\n",
    " n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  True\n",
      "[0.70852018 0.69438202] 0.7014511009220538\n"
     ]
    }
   ],
   "source": [
    "# Check to see if we got the same scores\n",
    "print(\"Scores Match: \", (scores == my_scores).all())\n",
    "print(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age: The Wrong Way\n",
    "This is a common enough mistake to warrant demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.699117647058763\n"
     ]
    }
   ],
   "source": [
    "# Imputation using the test set\n",
    "# This is \"test set leakage\"!\n",
    "# The test data was used to compute the mean.  This leads to overstating model accuracy.\n",
    "X.loc[X['Age'].isnull(), 'Age'] = X['Age'].mean()\n",
    "print(X['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "scores = cross_val_score(LogisticRegression(), X, y, cv=crossvalidation, scoring='accuracy',\n",
    " n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  False\n",
      "Test Leakage Score > Correct Score:  True\n",
      "[0.70627803 0.69662921] 0.7014536201944879\n"
     ]
    }
   ],
   "source": [
    "# Check to see if we got the same scores\n",
    "print(\"Scores Match: \", (scores == my_scores).all())\n",
    "print(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Test Set Null Age Values with Mean from *Train* Set\n",
    "This step is key to understanding how to avoid \"test set data leakage\".  If we look at the data in the test set, in any way, it no longer acts as a test set.\n",
    "\n",
    "We must replace null values in the test set with the mean from the *train* set without looking at any of the values in the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset will be used to evaluate the model's accuracy\n",
    "# Set the null Age values, in test, to the mean Age value, in train\n",
    "test_age_null = X_test['Age'].isnull()\n",
    "X_test.loc[test_age_null, 'Age'] = X_train['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Age, dtype: float64)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check that the values that were null are now the mean\n",
    "X_test.loc[test_age_null, 'Age'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Imputation\n",
    "It is critical to understand that we replaced null values in the test set with values computed from the train set.  We never looked at the data in test set\n",
    "\n",
    "Although we did not use Scikit Learn's imputer, when used properly, it also uses data from the train set to transform the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Datatypes\n",
    "This cell was copied from the 1st iteration.  In this notebook we are not going to address datatypes, but keep this note as a reminder (or put it in an issue tracking system).\n",
    "\n",
    "Based on a review of the data dictionary at [titanic](https://www.kaggle.com/c/titanic/data), and an examination of the values of each column, the following variables need to be converted to categorical:\n",
    "\n",
    "**Next Iteration: convert the following variables to categorical**\n",
    "- Pclass\n",
    "- Sex\n",
    "- Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2nd Iteration only, ignore all text and categorical variables\n",
    "X_train = X_train.drop('Pclass', axis=1)\n",
    "X_test = X_test.drop('Pclass', axis=1)\n",
    "X_train = X_train.drop('Name', axis=1)\n",
    "X_test = X_test.drop('Name', axis=1)\n",
    "X_train = X_train.drop('Sex', axis=1)\n",
    "X_test = X_test.drop('Sex', axis=1)\n",
    "X_train = X_train.drop('Ticket', axis=1)\n",
    "X_test = X_test.drop('Ticket', axis=1)\n",
    "X_train = X_train.drop('Embarked', axis=1)\n",
    "X_test = X_test.drop('Embarked', axis=1)\n",
    "X_train = X_train.drop('Cabin', axis=1)\n",
    "X_test = X_test.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question to ask is, wouldn't it have been easier to drop these columns prior\n",
    "to creating the train/test split so we wouldn't have to apply the same operation (drop column) to one?  The answer is \"yes\", but the proper way to do this, while ensuring no \"test data leakage\", is by way of pipelines and that will be discussed in a subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Fare           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the datatypes of each remaining column\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"preprocess\"></a>\n",
    "### Preprocessing\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "Preprocessing was done \"inline\" with the Exploratory Data Analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "### Model Building\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "base_model = LogisticRegression()\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eval\"></a>\n",
    "### Model Evaluation\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6753731343283582"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score will compute the accuarcy\n",
    "base_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67.5% is better than the previous iteration of 65.7%.  This may or may not be statistically significant.  A hypothesis test could be performed to see if it is, but that will not be done here.\n",
    "\n",
    "Here we will take a more simple approach.  The accuracy improved so we will tentatively continue to use Age Imputation as we iteratively Kaizen the model building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"summary\"></a>\n",
    "### Conclusion\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "In this iteration:\n",
    "* we showed an example of preprocessing without looking at the test set\n",
    "* measured an accuracy of 67.5% which is better than the previous iteration of 65.7%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
