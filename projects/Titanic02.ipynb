{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Model Dev 2 <br/>*Preprocessing and Cross Validation*\n",
    "\n",
    "Jupyter Notebook referenced from my website:\n",
    "[Software Nirvana: Iterative Dev 2](https://sdiehl28.netlify.com/2018/02/iterative-model-dev-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where We Are\n",
    "In the first iteration, we created a simple model and showed that the accuracy was better than the null model.  The null model is the model that predicts the predominant class in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "<a href=\"https://en.wikipedia.org/wiki/Imputation_(statistics)\">Imputation on Wikipedia</a>\n",
    "\n",
    "This notebook will impute the missing values for Age and use Age as an additional attribute for prediction.  We will also check to see if adding the Age variable improved prediction accuracy.\n",
    "\n",
    "Special attention will be paid to avoid a common beginner's mistake, which is to look at the test data when performing imputation or other preprocessing steps.  The easiest way to ensure there is no \"test set leakage\", is to use a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      int64\n",
       "Age       float64\n",
       "SibSp       int64\n",
       "Parch       int64\n",
       "Fare      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# break up the dataframe into X and y\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "\n",
    "# As before, remove all non-numeric fields and PassengerId\n",
    "drop_cols = ['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
    "X = X.drop(drop_cols, axis=1)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age: Cross Validation the Right Way\n",
    "This will be performed manually to emphasize how preprocessing operations work on the folds in a cross validation.  The key is that the held-out data, aka the test data, can never be looked at.\n",
    "\n",
    "This will also be performed with an Imputer and a Pipeline to show a more concise workflow.\n",
    "\n",
    "To a beginner, it can appear that using an Imputer and a Pipeline is a lot of extra work.  Why not just impute the Age before cross validation and be done with it.  If you were to look that at the [\"Kernels\"](https://www.kaggle.com/c/titanic/kernels?sortBy=votes&group=everyone&pageSize=20&competitionId=3136) on Kaggle posted for the Titantic dataset, you would see at least half the people do just that.  But this is wrong.  Looking at the test data prior to training your model is called \"data leakage\".\n",
    "\n",
    "Most beginners understand that the model will do better on the data it was trained on than out-of-sample data.  However if you use the test data prior to building your model, you are building your model on the test data.  This partially removes the benefit of having separate train and test data and the result may be too high an estimate of model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age without Pipeline\n",
    "The task here is to *correctly* impute missing Age values and then estimate model accuracy through K-fold Cross Validation.  The imputation will replace null Age values with the mean Age value.\n",
    "\n",
    "The key is to realize that we must perform imputation on both the missing values in the train fold and the missing values in the test fold.  Clearly we use the mean of the train data to replace the missing values in the train fold.  But what value to we use to replace the missing test missing?\n",
    "\n",
    "We use the mean of the *train* fold as the value to replace the missing *test* fold.  This is the key to understanding how to do Cross Validation correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random_state to get the same folds each time we call KFold()\n",
    "random_state = 121212\n",
    "\n",
    "# use low number of splits for illustration, 5 or 10 is the recommended value\n",
    "n_splits = 2\n",
    "\n",
    "# create K folds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the K folds to impute Age and compute the accuracy score\n",
    "my_scores = np.zeros(n_splits)\n",
    "i = 0\n",
    "lr_model = LogisticRegression()\n",
    "for train_idx, test_idx in crossvalidation.split(X):\n",
    "    # train subset\n",
    "    X_train = X.iloc[train_idx, :].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    \n",
    "    # test subset\n",
    "    X_test = X.iloc[test_idx, :].copy()\n",
    "    y_test = y[test_idx].copy()\n",
    "    \n",
    "    # find the average age on the train set\n",
    "    train_age_mean = X_train['Age'].mean()\n",
    "    \n",
    "    # use this value for *both* the train and test set\n",
    "    X_train.loc[X_train['Age'].isnull(), 'Age'] = train_age_mean\n",
    "    X_test.loc[X_test['Age'].isnull(), 'Age'] = train_age_mean # Key Concept!\n",
    "    \n",
    "    # fit model on train\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict using model on test\n",
    "    predictions = lr_model.predict(X_test)\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    my_scores[i] = accuracy_score(y_test, predictions)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Pandas copy\n",
    "In the above, we had to use .copy() for our train and test sets.  This is critical to avoiding the Pandas warning: SettingWithCopyWarning.\n",
    "\n",
    "The imputation requires us to modify both the train and test sets for X.  However X.iloc[] returns a *view* into X, not a copy of a subset of X.  If we try to use this view to modify data, we will get: SettingwithCopyWarning.\n",
    "\n",
    "In most cases, this warning means that your data will not be modified as you intend.  It is therefore important to write code that does not produce this warning.\n",
    "\n",
    "It can be difficult to see why this warning was issued.  The way to track this down is to print out the .is_copy member of your dataframe.  If it is a weak reference, then it is a view, otherwise it is an independent copy of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weakref at 0x7fe8ee1ed818; to 'DataFrame' at 0x7fe8f0d53ba8>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[train_idx, :].is_copy) # view into dataframe\n",
    "print(X.iloc[train_idx, :].copy().is_copy) # independent copy of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age with Pipeline\n",
    "Scikit Learn correctly uses the mean of the train set as the replacement value for null values in the test set.  However this is all done behind the scenes.  The following is exactly the same as the above, but requires much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: same as above\n",
    "random_state = 121212\n",
    "n_splits = 2\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  True\n",
      "[0.70852018 0.69438202] 0.7014511009220538\n"
     ]
    }
   ],
   "source": [
    "# Use an Imputer and a Pipeline\n",
    "# Note: Age is the only column in X with null values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='mean')\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "classifier = make_pipeline(imputer, LogisticRegression())\n",
    "\n",
    "# use cross_val_score() to properly compute the imputation and score per fold\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, scoring='accuracy',\n",
    " n_jobs=1)\n",
    "\n",
    "# Check to see that we got the same scores as in the above for loop\n",
    "print(\"Scores Match: \", (scores == my_scores).all())\n",
    "print(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both Methods Produce the Same Result\n",
    "The for loop over each K fold train/test splits is the same as using an Imputer and LogisticRegression in a Pipeline in cross_val_score().\n",
    "\n",
    "Observing matching scores does not in itself prove that the above two methods are the same, as this could occur by chance.  That said, the above code was written to ensure that both methods are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age: Cross Validation The Wrong Way\n",
    "In my review of Kaggle Kernels published for the Titantic dataset about half of the \"kernels\" made the following potentially serious mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to Cross Validation, impute the missing values as the mean of all the data\n",
    "# Don't do this!  This is \"data leakage\"!\n",
    "# Replace all null Age values with the mean of all Age Values\n",
    "X.loc[X['Age'].isnull(), 'Age'] = X['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: same as above\n",
    "random_state = 121212\n",
    "n_splits = 2\n",
    "crossvalidation = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross_val_score()\n",
    "scores = cross_val_score(LogisticRegression(), X, y, cv=crossvalidation, scoring='accuracy',\n",
    " n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Match:  False\n",
      "[0.70627803 0.69662921] 0.7014536201944879\n"
     ]
    }
   ],
   "source": [
    "# We do *not* get the same scores as above, because we did the cross validation wrong!\n",
    "print(\"Scores Match: \", (scores == my_scores).all())\n",
    "print(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing all the missing values, over both the train and test sets, prior to performing cross validation produced different results!\n",
    "\n",
    "For this dataset, I had to set n_splits=2, and try a few random_state values, before I got different results.  The effect for this dataset is subtle, but for the folds chosen, the difference was demonstrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation The Wrong Way: Discussion\n",
    "**What was wrong:** Used both train and test data for imputation. \n",
    "\n",
    "**What may happen:** Estimate of model accuracy may be too high.\n",
    "\n",
    "**Importance in Practice:** With imputation, this is often not much of an issue if the amount of data from which to perform the imputation is large. Nevertheless, there is no point in making a potentially serious error when using Pipelines makes it easy to do this correctly.\n",
    "\n",
    "**Common Real Life Situation with Serious Consequences:**\n",
    "If you have a lot of variables, and you decide, prior to performing cross validation, that you will remove some of the variables based on some statistic of the data, such as too much correlation with other variables, or too little correlation with the target variable, then you will almost certainly overfit your model.  That is, your report of model accuracy will be too high.\n",
    "\n",
    "**Great Explanation and Story by Robert Tibshirani:**\n",
    "Robert Tibshirani, in the youtube video [Cross Validation: Right and Wrong](https://www.youtube.com/watch?v=S06JpVoNaA0&list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf), explains the above in detail and presents a wonderful anecdotal story about a Ph.D. oral dissertation presenter preprocessing away variables prior to performing cross validation and the serious effect it had on his medical research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this iteration we:\n",
    "* showed the right and wrong way to perform cross validation was demonstrated\n",
    "* showed that using Pipelines with Cross Validation improves the quality of the software by making it easy to concisely perform cross validation correctly.\n",
    "* added the Age variable to our model and used it's mean to impute missing values\n",
    "* showed that our estimate of model accuracy increased from 68.5% to 70.1%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
