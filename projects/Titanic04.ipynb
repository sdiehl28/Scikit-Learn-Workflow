{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4: Custom Imputation with Pipelines\n",
    "\n",
    "Jupyter Notebook referenced from my website:: <a href=\"https://sdiehl28.netlify.com/projects/titanic/titanic03/\" target=\"_blank\">Software Nirvana: Titantic03</a>\n",
    "\n",
    "Compute Age based as median of passenger class.\n",
    "\n",
    "In this series of notebooks which demonstrates iterative model development, topics such as how to use Pandas will not be discussed.  However links to my Jupyter Notebooks which do discuss such topics will presented.\n",
    "    \n",
    "* [github repo](https://github.com/sdiehl28/tutorial-jupyter-notebooks)  \n",
    "* [Pandas: Series](http://nbviewer.jupyter.org/github/sdiehl28/tutorial-jupyter-notebooks/blob/master/pandas/Series.ipynb)  \n",
    "* [Pandas: Axis Specification](http://nbviewer.jupyter.org/github/sdiehl28/tutorial-jupyter-notebooks/blob/master/pandas/AxisSpecification.ipynb)  \n",
    "* [Pandas: DataFrame](http://nbviewer.jupyter.org/github/sdiehl28/tutorial-jupyter-notebooks/blob/master/pandas/Dataframe.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where We Are\n",
    "In the first iteration, we created a simple model and showed that the accuracy was better than the null model.  The null model is the model that predicts the predominant class in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next\n",
    "This notebook will focus on avoiding a common beginner's mistake, which is to look at the test data when performing imputation or any other preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"outline\"></a>\n",
    "### Outline\n",
    "1. [Previous Iteration](#previous)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "3. [Preprocessing](#preprocess)\n",
    "4. [Model Building](#model)\n",
    "5. [Model Evaluation](#eval)\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Software Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python:      3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) \n",
      "[GCC 7.2.0]\n",
      "numpy:       1.14.1\n",
      "pandas:      0.22.0\n",
      "matplotlib:  2.1.2\n",
      "seaborn:     0.8.1\n",
      "sklearn:     0.19.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('python:     ', sys.version)\n",
    "print('numpy:      ', np.__version__)\n",
    "print('pandas:     ', pd.__version__)\n",
    "import matplotlib\n",
    "print('matplotlib: ', matplotlib.__version__)\n",
    "print('seaborn:    ', sns.__version__)\n",
    "print('sklearn:    ', sk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"previous\"></a>\n",
    "### Previous Iteration\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (891, 11)\n",
      "y Shape:  (891,)\n"
     ]
    }
   ],
   "source": [
    "# break up the dataframe into X and y\n",
    "# X is a 2 dimensional \"spreadsheet\" of values used for prediction\n",
    "# y is a 1 dimensional vector of target (aka response) values\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "print('X Shape: ', X.shape)\n",
    "print('y Shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******\n",
    "### This Section Deals with a Subtle Train/Test Split Bug\n",
    "The train/test split should create *copies* of a dataframe rather than a view into a dataframe.  However in the version I am using, a view is returned for X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weakref at 0x7fc92d5b4908; to 'DataFrame' at 0x7fc92e1aceb8>\n",
      "<weakref at 0x7fc92d5b4908; to 'DataFrame' at 0x7fc92e1aceb8>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.is_copy)\n",
    "print(X_test.is_copy)\n",
    "print(y_train.is_copy)\n",
    "print(y_test.is_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see a weakref, this means we have a view.  That will cause problems with chained assignment later.  See: [Setting With Copy Warning](https://www.dataquest.io/blog/settingwithcopywarning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't have to do this, but it ensures we have independent copies\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X_train.is_copy)\n",
    "print(X_test.is_copy)\n",
    "print(y_train.is_copy)\n",
    "print(y_test.is_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is correct.  If DataFrame.is_copy is None, then we are not using a view into another DataFrame.\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eda\"></a>\n",
    "### Exploratory Data Analysis\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "One of the first things to check for is **null values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0.000000\n",
       "Pclass         0.000000\n",
       "Name           0.000000\n",
       "Sex            0.000000\n",
       "Age            0.187801\n",
       "SibSp          0.000000\n",
       "Parch          0.000000\n",
       "Ticket         0.000000\n",
       "Fare           0.000000\n",
       "Cabin          0.776886\n",
       "Embarked       0.003210\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the percentage of missing values per column\n",
    "nrows, ncols = X_train.shape\n",
    "X_train.isnull().sum() / nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Value Analysis\n",
    "The following is a reasonable judgement call as to how to proceed based on the observed percentages of null values.\n",
    "1. The Age attribute has some missing values => impute missing values\n",
    "2. Most of the Cabin attribute is missing => remove it\n",
    "3. Very few Emarked records are missing => remove records with missing Emarked value\n",
    "\n",
    "In this notebook we will perform Age Imputation but leave removing Emarked values for later.\n",
    "\n",
    "**Next Iteration:**\n",
    "- Removed records with missing Emarked value **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Age Value\n",
    "For instructional purposes, to emphasize not looking at the test data, this this will be performed manually rather than using a Scikit Learn Imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a boolean Series where Age is null\n",
    "train_age_null = X_train['Age'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573   NaN\n",
       "697   NaN\n",
       "601   NaN\n",
       "709   NaN\n",
       "783   NaN\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify we did this correctly\n",
    "X_train.loc[train_age_null, 'Age'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset will be used to train model\n",
    "# Set the null Age values, in train, to the mean Age value, in train\n",
    "X_train.loc[train_age_null, 'Age'] = X_train['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.787885375494064"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the mean value\n",
    "X_train['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573    29.787885\n",
       "697    29.787885\n",
       "601    29.787885\n",
       "709    29.787885\n",
       "783    29.787885\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check that the values that were null are now the mean\n",
    "X_train.loc[train_age_null, 'Age'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Test Set Null Age Values with Mean from *Train* Set\n",
    "This step is key to understanding how to avoid \"test set data leakage\".  If we look at the data in the test set, in any way, it no longer acts as a test set.\n",
    "\n",
    "We must replace null values in the test set with the mean from the *train* set without looking at any of the values in the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset will be used to evaluate the model's accuracy\n",
    "# Set the null Age values, in test, to the mean Age value, in train\n",
    "test_age_null = X_test['Age'].isnull()\n",
    "X_test.loc[test_age_null, 'Age'] = X_train['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584    29.787885\n",
       "411    29.787885\n",
       "826    29.787885\n",
       "384    29.787885\n",
       "692    29.787885\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check that the values that were null are now the mean\n",
    "X_test.loc[test_age_null, 'Age'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Imputation\n",
    "It is critical to understand that we replaced null values in the test set with values computed from the train set.  We never looked at the data in test set\n",
    "\n",
    "Although we did not use Scikit Learn's imputer, when used properly, it also uses data from the train set to transform the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Datatypes\n",
    "This cell was copied from the 1st iteration.  In this notebook we are not going to address datatypes, but keep this note as a reminder (or put it in an issue tracking system).\n",
    "\n",
    "Based on a review of the data dictionary at [titanic](https://www.kaggle.com/c/titanic/data), and an examination of the values of each column, the following variables need to be converted to categorical:\n",
    "\n",
    "**Next Iteration: convert the following variables to categorical**\n",
    "- Pclass\n",
    "- Sex\n",
    "- Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2nd Iteration only, ignore all text and categorical variables\n",
    "X_train = X_train.drop('Pclass', axis=1)\n",
    "X_test = X_test.drop('Pclass', axis=1)\n",
    "X_train = X_train.drop('Name', axis=1)\n",
    "X_test = X_test.drop('Name', axis=1)\n",
    "X_train = X_train.drop('Sex', axis=1)\n",
    "X_test = X_test.drop('Sex', axis=1)\n",
    "X_train = X_train.drop('Ticket', axis=1)\n",
    "X_test = X_test.drop('Ticket', axis=1)\n",
    "X_train = X_train.drop('Embarked', axis=1)\n",
    "X_test = X_test.drop('Embarked', axis=1)\n",
    "X_train = X_train.drop('Cabin', axis=1)\n",
    "X_test = X_test.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question to ask is, wouldn't it have been easier to drop these columns prior\n",
    "to creating the train/test split so we wouldn't have to apply the same operation (drop column) to one?  The answer is \"yes\", but the proper way to do this, while ensuring no \"test data leakage\", is by way of pipelines and that will be discussed in a subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Fare           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the datatypes of each remaining column\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"preprocess\"></a>\n",
    "### Preprocessing\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "Preprocessing was done \"inline\" with the Exploratory Data Analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "### Model Building\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "base_model = LogisticRegression()\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"eval\"></a>\n",
    "### Model Evaluation\n",
    "[Back to Outline](#outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[155,  69],\n",
       "       [ 16,  28]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6828358208955224\n"
     ]
    }
   ],
   "source": [
    "# Compute Accuracy\n",
    "base_model_accuracy = (155 + 28) / (155+69+16+28)\n",
    "print(base_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68.2% is about the same as the previous iteration of 67.9%.  The difference is probably not statistically significant, but the aim for this notebook was to show how to perform preprocessing without looking at the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"summary\"></a>\n",
    "### Conclusion\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "In this iteration:\n",
    "* we showed an example of preprocessing without looking at the test set\n",
    "* measured an accuracy of 68.2% which is \n",
    "* noted a few things to try to improve the model\n",
    "* established a baseline accuracy of 68%\n",
    "* showed that this accuracy is better than the null model accuracy of 64%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
