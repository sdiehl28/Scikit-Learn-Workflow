{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "Jupyter Notebook referenced from my website:\n",
    "[Software Nirvana: Baseline Model (1)](https://sdiehl28.netlify.com/2018/03/baseline-model-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "Discuss how to chose the \"best\" model between two models based on their Cross Validated Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Imports and Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "%matplotlib inline\n",
    "sns.set() # enable seaborn style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      int64\n",
       "SibSp       int64\n",
       "Parch       int64\n",
       "Fare      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the labeled data\n",
    "all_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# break up the dataframe into X and y\n",
    "X = all_data.drop('Survived', axis=1)\n",
    "y = all_data['Survived']\n",
    "\n",
    "# As before, remove all non-numeric fields and PassengerId\n",
    "drop_fields = ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'PassengerId']\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "\n",
    "# Remove all columns with null values (1st iteration only)\n",
    "X.dropna(axis=1, inplace=True)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"comparison\"></a>\n",
    "### Discussion\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "The purpose of Cross Validation is to estimate the model's accuracy on out-of-sample data.  This estimate can be used for determining:\n",
    "* how well the model will work when deployed to production\n",
    "* how to compare different models\n",
    "* how to compare the same model having different hyperparameters\n",
    "\n",
    "The final model deployed to production will be built using all the available data, as this produces the most accurate model.  But we cannot estimate the model's accuracy using all the data as this would lead to overfitting.  Model evaluation is best performed by evaluating the model on data it has never seen.\n",
    "\n",
    "Cross Validation may introduce some bias in its accuracy estimate, but this bias is usually similar for all models under consideration.  When used to compare the difference between the scores for two models, this bias largely cancels out, so cross validation is excellent for comparing the performance of different models.\n",
    "\n",
    "The bias in the accuracy estimate is in part due to the fact that the models are built on overlapping data and overlapping data is not independent.  This means that the 5 estimates in 5-fold cross validation are not 5 truly independent estimates but perhaps more like 3 or 4 independent estimates.  This also means that the computation of variance and standard deviation of the accuracy scores are too low.  This impact can be seen when the model is deployed to production and it performs less well than expected.  That said, except where there is a very large amount of data, the cross validated accuracy estimate is perhaps the best estimate available.\n",
    "\n",
    "Often the point-estimate of the Cross Validated score, such as the mean of the scores, is used to compare two models.  However if the standard deviation of the scores is comparable to the difference between the mean scores, then the lower score might be lower just by chance.  In some situations this is fine.  Picking one of two equally good models is as good as picking the other.\n",
    "\n",
    "In some situations, we would like to know if a proposed model change results in a model that is better by a statistically significant amount.  For example, a model change is proposed that will cost time and money to implement and we want to know beforehand if this cost is warranted.  And we may be looking for more than just statistical significance, but practical significance.  For example, we might want a hypothesis test that says the new model predicts with 3% better accuracy than the old model at the .05 significance level.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.666 0.708 0.673 0.694 0.686 0.679 0.695 0.676 0.686 0.688]\n",
      "Cross Validated Accuracy: 0.685  SD: 0.012\n"
     ]
    }
   ],
   "source": [
    "# Compute 5x2CV Scores for iteration 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "k_folds = 2\n",
    "repeats = 5\n",
    "random_seed = 108\n",
    "crossvalidation = RepeatedKFold(n_splits=k_folds, n_repeats=repeats, \n",
    "                                random_state=random_seed)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "scores = cross_val_score(classifier, X, y, cv=crossvalidation, \n",
    "                         scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print('Scores: ', np.round(scores, 3))\n",
    "print(f'Cross Validated Accuracy: {scores.mean():.3f}  SD: {scores.std():.3f}')\n",
    "\n",
    "# save these results so they can be used for comparison in future iterations\n",
    "np.save('../data/iter01.data', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"summary\"></a>\n",
    "### Summary\n",
    "[Back to Outline](#outline)\n",
    "\n",
    "In this first iteration we:\n",
    "* discussed how Scikit Learn computes accuracy, the confusion matrix, and cross validation scores\n",
    "* created a simple model to use as our baseline\n",
    "* established a baseline accuracy of 68.5%\n",
    "* discussed how to compare models by their cross validated scores\n",
    "* showed that our baseline model (68.5%) is better than the null model (61.6%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "51px",
    "right": "20px",
    "top": "142px",
    "width": "714px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
