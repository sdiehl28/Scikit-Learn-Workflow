{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions used by Baseball Notebooks\n",
    "\n",
    "**Baseball Notebooks**  \n",
    "1. Download and unzipped the Lahman and Retrosheet data.\n",
    "2. This notebook.\n",
    "\n",
    "The motivation for using the persistence functions follows.  \n",
    "The rest of the functions are self-explanatory.\n",
    "\n",
    "Note: %load <file.py>  \n",
    "will load the python code into the cell and comment out the %load line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Persistance Function Motivation\n",
    "\n",
    "**Primary Concern: Column Data Types**\n",
    "* dates should be read back in as dates to allow for date specific operations\n",
    "* small integer values should be read back as small integers to save memory\n",
    "* categories should be read back as categories to save memory  \n",
    "  \n",
    "In general it is helpful to both data analysts and other software libraries which may make use of the data, to know the most specific data type for a variable.\n",
    "\n",
    "**Persisted Data Representations**  \n",
    "For demonstration purposes, the data will be persisted in two forms (only one form is necessary for data analysis):\n",
    "1. CSV files\n",
    "2. Postgres Tables\n",
    "\n",
    "**CSV Files**  \n",
    "To avoid loss of Pandas column data types:\n",
    "* When writing:\n",
    "  * The DataFrame's data types will be written to a csv file.\n",
    "  * The DataFrame will be written to a csv file.\n",
    "* When reading:\n",
    "  * The data types will be read.\n",
    "  * The data types will be used to properly read in the DataFrame.\n",
    "\n",
    "**Postgres**  \n",
    "1. pd.to_sql()\n",
    "  * will choose correct Postgres data type for pd.datetime\n",
    "  * will choose Postgres Big Integer for all Pandas integer columns\n",
    "    * => specify SQL Alchemy types such as SmallInteger and Integer, appropriately\n",
    "2. df.read_sql()\n",
    "  * will choose correct Pandas data type for Postgres timestamp column\n",
    "  * will choose np.int64 data type for Postgres integers\n",
    "    * => optionally use pd.to_numeric with downcast\n",
    "  * will choose object when category may be more appropriate\n",
    "    * => optionally convert object to category\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Persistance Function Usage \n",
    "1. Read in csv data with Pandas.\n",
    "2. Wrangle the data.\n",
    "3. \"Optimize\" the DataFrame data types with: df_optimized = optimize_df_types(df)\n",
    "4. \"Optimize\" the Database data types with: dtypes = optimize_db_types(df_optimized)\n",
    "5. Write with: to_csv_with_types()\n",
    "6. Read back with: from_csv_with_types()\n",
    "7. Write with: df.to_sql(dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load helper_functions.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy.types import SmallInteger, Integer, BigInteger\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def to_csv_with_types(df, filename):\n",
    "    \"\"\"\n",
    "    Save df to csv file and save df.dtypes to csv file.\n",
    "\n",
    "    If filename ends in .gz, Pandas will use gzip compression.\n",
    "\n",
    "    This is intended to be used after optimizing df column types.\n",
    "    Read back with: from_csv_with_types()\n",
    "    \"\"\"\n",
    "\n",
    "    filename_types = filename.split('.')[0] + '_types.csv'\n",
    "    dtypes = df.dtypes.to_frame('dtypes').reset_index()\n",
    "\n",
    "    dtypes.to_csv(filename_types, index=False)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def from_csv_with_types(filename, nrows=None):\n",
    "    \"\"\"\n",
    "    Read df.dtypes from csv file and read df from csv file.\n",
    "\n",
    "    If filename ends in .gz, Pandas will use gzip decompression.\n",
    "    This is the complement of to_csv_with_types().\n",
    "    \"\"\"\n",
    "\n",
    "    filename_types = filename.split('.')[0] + '_types.csv'\n",
    "\n",
    "    types = pd.read_csv(filename_types).set_index('index').to_dict()\n",
    "    dtypes = types['dtypes']\n",
    "\n",
    "    dates = [key for key, value in dtypes.items() if value.startswith('datetime')]\n",
    "    for field in dates:\n",
    "        dtypes.pop(field)\n",
    "\n",
    "    return pd.read_csv(filename, parse_dates=dates, dtype=dtypes, nrows=nrows)\n",
    "\n",
    "\n",
    "def optimize_df_dtypes(df, ignore=None):\n",
    "    \"\"\"\n",
    "    Downcasts DataFrame Column Types.\n",
    "\n",
    "    :param df:\n",
    "    Dataframe to optimize.\n",
    "\n",
    "    :param cutoff:\n",
    "    Specifies cutoff ratio of unique values to rows for converting to categories.\n",
    "\n",
    "    :param ignore\n",
    "    Specifies which fields to exclude from downcasting.\n",
    "\n",
    "    :return:\n",
    "    Optimized DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # columns to consider for downcasting\n",
    "    process_cols = df.columns\n",
    "    if ignore and len(ignore) > 0:\n",
    "        process_cols = df.columns.difference(ignore)\n",
    "\n",
    "        if len(process_cols) == 0:\n",
    "            return df\n",
    "\n",
    "    # get the integer columns, if any\n",
    "    df_int = df[process_cols].select_dtypes(include=[np.int])\n",
    "\n",
    "    # if there are some integer columns, downcast them\n",
    "    if len(df_int.columns) > 0:\n",
    "        df_int = df_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "        df[df_int.columns] = df_int\n",
    "\n",
    "    # automated conversion to categories can be problematic\n",
    "    # if a category is warranted, probably a CategoryDType should be created\n",
    "\n",
    "    # get the object columns, if any\n",
    "    # df_obj = df[process_cols].select_dtypes(include=['object'])\n",
    "    #\n",
    "    # # if there are some object columns, convert to category if less than 10% unique\n",
    "    # if len(df_obj.columns) > 0:\n",
    "    #     s = df_obj.nunique() / df.shape[0]\n",
    "    #     columns = s.index[s <= cutoff].values\n",
    "    #     if len(columns) > 0:\n",
    "    #         df_cat = df[columns].astype('category')\n",
    "    #         df[columns] = df_cat\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_db_dtypes(df):\n",
    "    \"\"\"\n",
    "    Choose smallest ANSI SQL Column Type for integer that fits the optimized DataFrame.\n",
    "\n",
    "    Relies on:\n",
    "    from sqlalchemy.types import SmallInteger, Integer, BigInteger\n",
    "    \"\"\"\n",
    "    small_int = {col: SmallInteger for col in df.select_dtypes(\n",
    "        include=[np.int16, np.uint16, np.int8, np.uint8]).columns}\n",
    "\n",
    "    integer = {col: Integer for col in df.select_dtypes(\n",
    "        include=[np.int32, np.uint32]).columns}\n",
    "\n",
    "    big_int = {col: BigInteger for col in df.select_dtypes(\n",
    "        include=[np.int64, np.uint64]).columns}\n",
    "\n",
    "    dtypes = {**small_int, **integer, **big_int}\n",
    "\n",
    "    return dtypes\n",
    "\n",
    "def mem_usage(df):\n",
    "    \"\"\"Returns a string representing df memory usage in MB.\"\"\"\n",
    "    mem = df.memory_usage(deep=True).sum()\n",
    "    mem = mem / 2 ** 20  # covert to megabytes\n",
    "    return f'{mem:03.2f} MB'\n",
    "\n",
    "\n",
    "def is_int(s):\n",
    "    \"\"\"Returns True if all non-null values are integers.\n",
    "\n",
    "    Useful for determining if the df column (pd.Series) is\n",
    "    float just to hold missing values.\n",
    "    \"\"\"\n",
    "    notnull = s.notnull()\n",
    "    is_integer = s.apply(lambda x: (x % 1 == 0.0))\n",
    "    return (notnull == is_integer).all()\n",
    "\n",
    "\n",
    "def convert_camel_case(name):\n",
    "    \"\"\"\n",
    "    CamelCase to snake_case.\n",
    "\n",
    "    This is from:\n",
    "    https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case#answer-1176023\n",
    "    \"\"\"\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def is_unique(df, cols):\n",
    "    \"\"\"Fast determination of multi-column uniqueness.\"\"\"\n",
    "    return not (df.duplicated(subset=cols)).any()\n",
    "\n",
    "def game_id_to_url(game_id):\n",
    "    home = game_id[:3]\n",
    "    url = 'https://www.baseball-reference.com/boxes/' + home + '/' + game_id + '.shtml'\n",
    "    display(HTML(f'<a href=\"{url}\">{game_id}</a>'))\n",
    "\n",
    "def order_cols(df,cols):\n",
    "    \"\"\"Put columns in cols first, followed by rest of columns\"\"\"\n",
    "    rest = [col for col in df.columns if col not in cols]\n",
    "    df = df[cols + rest]\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
